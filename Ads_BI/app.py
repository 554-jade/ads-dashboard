import streamlit as st
import pandas as pd
import plotly.express as px
import gspread
from google.oauth2.service_account import Credentials
from urllib.parse import urlparse
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
import os
import datetime

# -----------------------------------------------------------------------------
# é…ç½®ä¸è¯´æ˜
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="Antigravity Ads Cloud - å¹¿å‘Šäº‘",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¤è¯é…ç½®
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]


from sqlalchemy import create_engine, text

# -----------------------------------------------------------------------------
# æ•°æ®åŠ è½½ä¸å¤„ç† (ETL)
# -----------------------------------------------------------------------------

def get_gspread_client():
    """ä½¿ç”¨ Streamlit secrets è¿›è¡Œ Google Sheets è®¤è¯"""
    try:
        creds_dict = dict(st.secrets["connections"]["gsheets"])
        creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        client = gspread.authorize(creds)
        return client
    except Exception as e:
        st.error(f"è®¤è¯é”™è¯¯: {e}")
        st.stop()

def get_db_connection():
    """Create MySQL connection using SQLAlchemy"""
    try:
        db_config = st.secrets["connections"]["mysql"]
        # Format: mysql+pymysql://user:password@host:port/database
        connection_str = f"mysql+pymysql://{db_config['username']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}"
        engine = create_engine(connection_str)
        return engine.connect()
    except Exception as e:
        st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        st.stop()




@st.cache_data(ttl=600)
def load_data():
    """
    ä» MySQL åŠ è½½ Campaign çº§åˆ«æ•°æ® (t_google_cost)ï¼Œå¹¶è¯»å–æœ¬åœ° Excel æ˜ å°„è¡¨ã€‚
    """
    # 1. åŠ è½½æ˜ å°„è¡¨ (From Local Excel)
    manager_map = pd.DataFrame()
    
    excel_path = "Ads_BI/mapping.xlsx"
    try:
        # Load Manager Map
        # å‡è®¾ Excel ä¸­æœ‰åä¸º "Manager_Map" çš„ sheetï¼Œæˆ–è€…æˆ‘ä»¬è¯»å–ç¬¬ä¸€ä¸ªåŒ…å« "å¹¿å‘Šè´¦å·" çš„ sheet
        xls = pd.ExcelFile(excel_path)
        sheet_names = xls.sheet_names
        
        target_sheet = None
        for s in sheet_names:
            if "manager" in s.lower() or "ä¼˜åŒ–å¸ˆ" in s:
                target_sheet = s
                break
        if not target_sheet and sheet_names:
            target_sheet = sheet_names[0] # Fallback
            
        if target_sheet:
            manager_map = pd.read_excel(xls, sheet_name=target_sheet)
            # Normalize Columns
            manager_map.columns = [c.strip() for c in manager_map.columns]
            # Ensure required columns exist
            if 'å¹¿å‘Šè´¦å·' in manager_map.columns and 'ä¼˜åŒ–å¸ˆ' in manager_map.columns:
                 # Standardize ID: remove all non-digits for robust matching
                 manager_map['join_id'] = manager_map['å¹¿å‘Šè´¦å·'].astype(str).str.replace(r'\D', '', regex=True)
                 # å…³é”®ï¼šå»é‡ï¼Œé˜²æ­¢å¦‚æœæ˜ å°„è¡¨é‡ŒåŒä¸€ä¸ªè´¦å·å‡ºç°å¤šæ¬¡ï¼Œå¯¼è‡´åˆå¹¶åçš„æ•°æ®ç¿»å€
                 manager_map = manager_map.drop_duplicates(subset=['join_id'])
            else:
                 st.warning(f"æ˜ å°„è¡¨ {target_sheet} ç¼ºå°‘ 'å¹¿å‘Šè´¦å·' æˆ– 'ä¼˜åŒ–å¸ˆ' åˆ—")
                 manager_map = pd.DataFrame()
        
    except Exception as e:
        st.error(f"åŠ è½½æœ¬åœ°æ˜ å°„è¡¨å¤±è´¥: {e}")
    

    # 1.2 åŠ è½½ Campaign -> URL / è½åœ°é¡µ / ç±»ç›® æ˜ å°„è¡¨ (Bridge Map)
    bridge_map = {}
    landing_page_map = {}
    category_direct_map = {}
    
    try:
        # Look for sheet "å¹¿å‘Šmapping"
        if "å¹¿å‘Šmapping" in sheet_names:
            bridge_df = pd.read_excel(xls, sheet_name="å¹¿å‘Šmapping")
            bridge_df.columns = [c.strip() for c in bridge_df.columns]
            
            # --- å¼ºåŠ›æ¸…æ´— (Deep Cleaning) ---
            if 'å¹¿å‘Šç³»åˆ—' in bridge_df.columns:
                # 1. å¤„ç† Excel åˆå¹¶å•å…ƒæ ¼ (Forward Fill)
                bridge_df['å¹¿å‘Šç³»åˆ—'] = bridge_df['å¹¿å‘Šç³»åˆ—'].ffill()
                # 2. å½’ä¸€åŒ–: è½¬å°å†™ + å»é¦–å°¾ç©ºæ ¼ + è§„èŒƒåŒ–ä¸­é—´ç©ºæ ¼ (æŠŠå¤šä¸ªç©ºæ ¼å˜ä¸€ä¸ª)
                bridge_df['å¹¿å‘Šç³»åˆ—'] = bridge_df['å¹¿å‘Šç³»åˆ—'].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip().str.lower()
            # ------------------------------

            # Campaign -> URL
            if 'å¹¿å‘Šç³»åˆ—' in bridge_df.columns and 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in bridge_df.columns:
                temp_df = bridge_df.dropna(subset=['å¹¿å‘Šç³»åˆ—'])
                # å¹¿å‘Šç³»åˆ—å·²åœ¨ä¸Šé¢å…¨å±€æ¸…æ´—è¿‡ï¼Œæ— éœ€é‡å¤æ¸…æ´—
                temp_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'] = temp_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].astype(str)
                # Aggregate multiple URLs to prevent overwriting
                bridge_map = temp_df.groupby('å¹¿å‘Šç³»åˆ—')['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(lambda x: ' | '.join(x.unique())).to_dict()
                
            # Campaign -> è½åœ°é¡µ (Landing Page)
            if 'å¹¿å‘Šç³»åˆ—' in bridge_df.columns and 'è½åœ°é¡µ' in bridge_df.columns:
                temp_df = bridge_df.dropna(subset=['å¹¿å‘Šç³»åˆ—'])
                # å¹¿å‘Šç³»åˆ—å·²å…¨å±€æ¸…æ´—
                # Ensure Landing Page is string
                temp_df['è½åœ°é¡µ'] = temp_df['è½åœ°é¡µ'].fillna("").astype(str)
                # Aggregate multiple Landing Pages
                landing_page_map = temp_df.groupby('å¹¿å‘Šç³»åˆ—')['è½åœ°é¡µ'].apply(lambda x: ' | '.join([v for v in x.unique() if v])).to_dict()

            # Campaign -> ç±»ç›® (Category) - DIRECT MAPPING
            if 'å¹¿å‘Šç³»åˆ—' in bridge_df.columns and 'ç±»ç›®' in bridge_df.columns:
                temp_df = bridge_df.dropna(subset=['å¹¿å‘Šç³»åˆ—'])
                # å¹¿å‘Šç³»åˆ—å·²å…¨å±€æ¸…æ´—
                # Ensure Category is string
                temp_df['ç±»ç›®'] = temp_df['ç±»ç›®'].fillna("Unknown").astype(str)
                # Aggregate multiple Categories (though usually 1, safety first)
                category_direct_map = temp_df.groupby('å¹¿å‘Šç³»åˆ—')['ç±»ç›®'].apply(lambda x: ' | '.join(x.unique())).to_dict()
                
    except Exception as e:
         st.warning(f"åŠ è½½å¹¿å‘Šæ˜ å°„è¡¨å¤±è´¥: {e}")

    # 1.3 (Old Category Logic Removed/Disabled)
    # ...
    
    # ... (Lines 137-189 skipped for brevity, make sure to keep correct context if jump is large. 
    # Actually tool requires contiguous block or separate calls. 
    # The user wants to map "dildo" correctly. The prompt asks to fixing the "only 3 results" issue.
    # I need to update the lookup side as well.
    # To do this in one go with replace_file_content is hard if the lines are far apart (115 vs 191).
    # I will split into two edits.)
    
    # EDIT 1: Update Map Creation (Lines ~112-130)

                
    except Exception as e:
         st.warning(f"åŠ è½½å¹¿å‘Šæ˜ å°„è¡¨å¤±è´¥: {e}")

    # 1.3 (Old Category Logic Removed/Disabled as per request to use 'å¹¿å‘Šmapping')
    # category_map_dict = {} ...
         
    
    # 2. åŠ è½½å¹¿å‘Šæ•°æ® (From MySQL - Campaign Level)
    try:
        conn = get_db_connection()
        # t_google_cost schema: day_time, customer_id, campaign_name, cost, conversions, all_conversion_value
        query = """
            SELECT 
                day_time as 'å¤©',
                customer_id as 'å¹¿å‘Šè´¦å·',
                campaign_name as 'å¹¿å‘Šç³»åˆ—',
                cost as 'è´¹ç”¨',
                conversions as 'è½¬åŒ–æ•°',
                conversions_value_by_conversion_date as 'è½¬åŒ–ä»·å€¼'
            FROM t_google_cost
            WHERE day_time >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)
        """
        raw_df = pd.read_sql(text(query), conn)
        conn.close()
        
        if raw_df.empty:
            return pd.DataFrame()

        # æ•°æ®ç±»å‹è½¬æ¢ä¸æ¸…æ´—
        raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'])
        
        numeric_cols = ['è´¹ç”¨', 'è½¬åŒ–æ•°', 'è½¬åŒ–ä»·å€¼']
        for col in numeric_cols:
            raw_df[col] = pd.to_numeric(raw_df[col], errors='coerce').fillna(0)
            
        # å¼ºåˆ¶è®¡ç®— ROAS
        raw_df['ROAS'] = raw_df.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)
        
        # Standardize ID for joining
        raw_df['join_id'] = raw_df['å¹¿å‘Šè´¦å·'].astype(str).str.replace(r'\D', '', regex=True)

    except Exception as e:
        st.error(f"æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
        return pd.DataFrame()

    # 3. æ•°æ®åˆå¹¶
    
    # 3.1 ä¼˜åŒ–å¸ˆæ˜ å°„ (Manager)
    if not manager_map.empty:
        map_dedup = manager_map[['join_id', 'ä¼˜åŒ–å¸ˆ']].drop_duplicates(subset=['join_id'])
        merged_df = pd.merge(raw_df, map_dedup, on='join_id', how='left')
        merged_df['ä¼˜åŒ–å¸ˆ'] = merged_df['ä¼˜åŒ–å¸ˆ'].fillna("Unknown")
    else:
        merged_df = raw_df.copy()
        merged_df['ä¼˜åŒ–å¸ˆ'] = "Unknown"

    

    
    # 3.2 ç±»ç›®æ˜ å°„ (Category) - DIRECT FROM MAPPING SHEET
    # Step A: Map Campaign -> URL & Landing Page & Category
    def get_url_from_campaign(row):
        # Key normalization: Collapse multiple spaces, strip, lower to match map keys
        # "foo  bar" -> "foo bar"
        camp_name = ' '.join(str(row.get('å¹¿å‘Šç³»åˆ—', '')).split()).lower()
        return bridge_map.get(camp_name, "")
    
    def get_lp_from_campaign(row):
        # Key normalization
        camp_name = ' '.join(str(row.get('å¹¿å‘Šç³»åˆ—', '')).split()).lower()
        return landing_page_map.get(camp_name, "")
        
    def get_cat_from_campaign(row):
        # Key normalization
        camp_name = ' '.join(str(row.get('å¹¿å‘Šç³»åˆ—', '')).split()).lower()
        # Strictly use mapping sheet
        return category_direct_map.get(camp_name, "Unknown")

    merged_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'] = merged_df.apply(get_url_from_campaign, axis=1)
    merged_df['è½åœ°é¡µ'] = merged_df.apply(get_lp_from_campaign, axis=1)
    merged_df['ç±»ç›®'] = merged_df.apply(get_cat_from_campaign, axis=1)


    

    # 3.3 è¡¥å…¨ç¼ºå¤±åˆ—ä»¥å…¼å®¹åç»­é€»è¾‘
    merged_df['å¹¿å‘Šç»„'] = "All"
    merged_df['clean_url'] = ""

    # 3.4 ç”Ÿæˆ ID
    merged_df['å¹¿å‘Šç»„id'] = (
        merged_df['å¹¿å‘Šè´¦å·'].astype(str) + "_" +
        merged_df['ç±»ç›®'].astype(str) + "_" +  # Added Category to ID for uniqueness
        merged_df['å¹¿å‘Šç³»åˆ—'].astype(str)
    )

    return merged_df




# ä¸»åº”ç”¨ç¨‹åº
# -----------------------------------------------------------------------------

def main():
    st.title("Antigravity Ads Cloud ğŸš€ - å¹¿å‘Šäº‘")
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    if df.empty:
        st.warning("æœªæ‰¾åˆ°æ•°æ®æˆ–è¿æ¥å¤±è´¥ã€‚å¯èƒ½æ˜¯ç½‘ç»œæ³¢åŠ¨ï¼Œè¯·å°è¯•åˆ·æ–°ã€‚")
        if st.button("ğŸ”„ é‡è¯•è¿æ¥ (Retry)"):
            st.cache_data.clear()
            st.rerun()
        return

    # -------------------------------------------------------------------------
    # ä¾§è¾¹æ ç­›é€‰å™¨
    # -------------------------------------------------------------------------
    st.sidebar.header("å…¨å±€ç­›é€‰å™¨")
    
    if st.sidebar.button("ğŸ”„ åˆ·æ–°æ•°æ® (é‡ç½®ç¼“å­˜)"):
        st.cache_data.clear()
        st.rerun()
    
    min_date = df['å¤©'].min().date()
    max_date = df['å¤©'].max().date()
    start_date = st.sidebar.date_input("å¼€å§‹æ—¥æœŸ", value=max(min_date, max_date - pd.Timedelta(days=30)))
    end_date = st.sidebar.date_input("ç»“æŸæ—¥æœŸ", value=max_date)

    mask_date = (df['å¤©'].dt.date >= start_date) & (df['å¤©'].dt.date <= end_date)
    df_filtered_date = df[mask_date]

    managers = ["æ•´ä½“"] + sorted(df_filtered_date['ä¼˜åŒ–å¸ˆ'].unique().tolist())
    selected_managers = st.sidebar.multiselect("ä¼˜åŒ–å¸ˆ", managers, default=["æ•´ä½“"])
    
    if "æ•´ä½“" in selected_managers:
        df_filtered_manager = df_filtered_date
    else:
        df_filtered_manager = df_filtered_date[df_filtered_date['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)]
    
    categories = sorted(df_filtered_manager['ç±»ç›®'].astype(str).unique().tolist())
    selected_categories = st.sidebar.multiselect("ç±»ç›®", categories, default=categories)
    
    df_filtered_category = df_filtered_manager[df_filtered_manager['ç±»ç›®'].isin(selected_categories)]
    
    accounts = sorted(df_filtered_category['å¹¿å‘Šè´¦å·'].unique().tolist())
    selected_accounts = st.sidebar.multiselect("å¹¿å‘Šè´¦å·", accounts, default=accounts)
    
    final_df = df_filtered_category[df_filtered_category['å¹¿å‘Šè´¦å·'].isin(selected_accounts)]



    # -------------------------------------------------------------------------
    # Tabs
    # -------------------------------------------------------------------------
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "æŒ‡æŒ¥ä¸­å¿ƒ", "å›¢é˜Ÿä¸æˆ˜ç•¥", "æ·±åº¦é€è§†", "çº¢é»‘æ¦œ (å¼‚å¸¸è¯Šæ–­)", "æ•°æ®ä»“åº“", "ä¼˜åŒ–å¸ˆç›®æ ‡ç®¡ç†"
    ])

    with tab1:
        st.subheader("æŒ‡æŒ¥ä¸­å¿ƒ (Command Center)")
        
        current_spend = final_df['è´¹ç”¨'].sum()
        current_val = final_df['è½¬åŒ–ä»·å€¼'].sum()
        current_roas = current_val / current_spend if current_spend > 0 else 0
        
        has_conversions = 'è½¬åŒ–æ•°' in final_df.columns
        current_conversions = final_df['è½¬åŒ–æ•°'].sum() if has_conversions else 1
        current_cpa = current_spend / current_conversions if has_conversions and current_conversions > 0 else 0

        days_diff = (end_date - start_date).days + 1
        prev_start = start_date - pd.Timedelta(days=days_diff)
        prev_end = start_date - pd.Timedelta(days=1)
        
        mask_prev = (df['å¤©'].dt.date >= prev_start) & (df['å¤©'].dt.date <= prev_end) & \
                    (df['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)) & \
                    (df['ç±»ç›®'].isin(selected_categories)) & \
                    (df['å¹¿å‘Šè´¦å·'].isin(selected_accounts))
        prev_df = df[mask_prev]
        
        prev_spend = prev_df['è´¹ç”¨'].sum()
        prev_val = prev_df['è½¬åŒ–ä»·å€¼'].sum()
        prev_roas = prev_val / prev_spend if prev_spend > 0 else 0
        
        permil_delta_spend = ((current_spend - prev_spend) / prev_spend) * 100 if prev_spend > 0 else 0
        permil_delta_roas = ((current_roas - prev_roas) / prev_roas) * 100 if prev_roas > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        col1.metric("æ€»æ¶ˆè€— (Total Spend)", f"${current_spend:,.2f}", f"{permil_delta_spend:.2f}%")
        col2.metric("æ•´ä½“ ROAS", f"{current_roas:.2f}", f"{permil_delta_roas:.2f}%")
        col3.metric("æ€»è½¬åŒ–ä»·å€¼ (Total Value)", f"${current_val:,.2f}") 

        st.markdown("### ä¸šç»©è¶‹åŠ¿ (Performance Trend)")
        daily_trend = final_df.groupby('å¤©').agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        daily_trend['ROAS'] = daily_trend['è½¬åŒ–ä»·å€¼'] / daily_trend['è´¹ç”¨']
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(
            go.Bar(x=daily_trend['å¤©'], y=daily_trend['è´¹ç”¨'], name="æ¶ˆè€— (Spend)"),
            secondary_y=False,
        )
        fig.add_trace(
            go.Scatter(x=daily_trend['å¤©'], y=daily_trend['ROAS'], name="ROAS", mode='lines+markers'),
            secondary_y=True,
        )
        fig.update_layout(title_text="æ¶ˆè€— vs ROAS è¶‹åŠ¿")
        fig.update_yaxes(title_text="æ¶ˆè€— (Spend)", secondary_y=False)
        fig.update_yaxes(title_text="ROAS", secondary_y=True)
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        st.subheader("å›¢é˜Ÿä¸æˆ˜ç•¥ (Team & Strategy)")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### äººæ•ˆçŸ©é˜µ (People Matrix)")
            manager_perf = final_df.groupby('ä¼˜åŒ–å¸ˆ').agg({
                'è´¹ç”¨': 'sum', 
                'è½¬åŒ–ä»·å€¼': 'sum'
            }).reset_index()
            manager_perf['ROAS'] = manager_perf['è½¬åŒ–ä»·å€¼'] / manager_perf['è´¹ç”¨']
            
            fig_bubble = px.scatter(
                manager_perf, x="è´¹ç”¨", y="ROAS", size="è½¬åŒ–ä»·å€¼", color="ä¼˜åŒ–å¸ˆ",
                hover_name="ä¼˜åŒ–å¸ˆ", title="ä¼˜åŒ–å¸ˆè¡¨ç°çŸ©é˜µ", size_max=60
            )
            fig_bubble.add_hline(y=1.0, line_dash="dash", line_color="red")
            st.plotly_chart(fig_bubble, use_container_width=True)
            
        with col2:
            st.markdown("#### å“ç±»ç‰ˆå›¾ (Category Share)")
            cat_perf = final_df.groupby('ç±»ç›®').agg({'è´¹ç”¨': 'sum'}).reset_index()
            fig_pie = px.pie(cat_perf, values='è´¹ç”¨', names='ç±»ç›®', title="å„å“ç±»æ¶ˆè€—å æ¯”")
            st.plotly_chart(fig_pie, use_container_width=True)

    with tab3:
        st.subheader("æ·±åº¦é€è§† (Deep Pivot)")
        c1, c2 = st.columns(2)
        with c1:
            pivot_rows = st.multiselect("è¡Œç»´åº¦", ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¤©', 'å¹¿å‘Šè´¦å·', 'å¹¿å‘Šç³»åˆ—', 'è½åœ°é¡µ'], default=['ä¼˜åŒ–å¸ˆ'])
        with c2:
            pivot_vals = st.multiselect("æ•°å€¼æŒ‡æ ‡", ['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'], default=['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'])
            
        if pivot_rows and pivot_vals:
            pivot_df = final_df.groupby(pivot_rows)[['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼']].sum().reset_index()
            pivot_df['ROAS'] = pivot_df['è½¬åŒ–ä»·å€¼'] / pivot_df['è´¹ç”¨']

            # å¤„ç† ROAS å¯èƒ½äº§ç”Ÿçš„æ— é™å€¼ (Divide by zero)
            import numpy as np
            pivot_df = pivot_df.replace([np.inf, -np.inf], 0)



            st.markdown("###### ğŸ”½ ç»™é€è§†è¡¨ä½“æ£€ (Pivot Filters)")
            
            # 1. ç»´åº¦ç­›é€‰ (Dimension Filters)
            if len(pivot_rows) > 0:
                d_cols = st.columns(len(pivot_rows))
                for i, col_key in enumerate(pivot_rows):
                    with d_cols[i]:
                        # æ”¹ä¸ºæ–‡æœ¬æœç´¢æ¡† (Fuzzy Search)
                        search_term = st.text_input(f"ğŸ” {col_key}", key=f"p_filter_{col_key}", placeholder="è¾“å…¥å…³é”®è¯...")
                        if search_term:
                            # æ¨¡ç³ŠåŒ¹é…ï¼šä¸åŒºåˆ†å¤§å°å†™
                            pivot_df = pivot_df[pivot_df[col_key].astype(str).str.contains(search_term, case=False, na=False)]
            
            # 2. æ•°å€¼æŒ‡æ ‡ç­›é€‰ (Metric Filters) - æ”¾å…¥æŠ˜å é¢æ¿ä»¥å‡å°‘å¹²æ‰°
            if len(pivot_vals) > 0:
                with st.expander("ğŸ”¢ æ•°å€¼èŒƒå›´ç­›é€‰ (Numeric Filters)", expanded=False):
                    m_cols = st.columns(len(pivot_vals))
                    for i, col_key in enumerate(pivot_vals):
                        with m_cols[i]:
                            # æ ‡é¢˜åŠ ç²—ï¼Œæ¸…æ™°åŒºåˆ†æŒ‡æ ‡
                            st.markdown(f"**{col_key}**")
                            # è·å–çœŸå®æ•°æ®çš„è¾¹ç•Œ
                            real_min = float(pivot_df[col_key].min())
                            real_max = float(pivot_df[col_key].max())
                            
                            # é»˜è®¤æ˜¾ç¤ºé€»è¾‘ï¼š
                            # Min é»˜è®¤ä¸º 0 (çœ‹èµ·æ¥åƒ"æ— ç­›é€‰")ï¼Œé™¤éçœŸå®æœ€å°å€¼æ˜¯è´Ÿæ•°
                            default_min = 0.0 if real_min >= 0 else real_min
                            # Max é»˜è®¤ä¸ºçœŸå®æœ€å¤§å€¼
                            default_max = real_max

                            step_v = 0.01 if 'ROAS' in col_key else 100.0
                            
                            c_min, c_max = st.columns(2)
                            with c_min:
                                val_min = st.number_input("Min", value=default_min, step=step_v, key=f"min_{col_key}")
                            with c_max:
                                val_max = st.number_input("Max", value=default_max, step=step_v, key=f"max_{col_key}")
                            
                            # åº”ç”¨ç­›é€‰
                            pivot_df = pivot_df[(pivot_df[col_key] >= val_min) & (pivot_df[col_key] <= val_max)]

            display_cols = pivot_rows + pivot_vals
            styler = pivot_df[display_cols].style
            styler = styler.format("{:.2f}", subset=pivot_vals)
            if 'ROAS' in display_cols:
                styler = styler.background_gradient(subset=['ROAS'], cmap="RdYlGn", vmin=0.5, vmax=2.0)
            
            st.dataframe(
                styler,
                use_container_width=True,
                height=500  # å›ºå®šé«˜åº¦é˜²æ­¢æ•°æ®ç­›é€‰æ—¶è¡¨æ ¼è·³åŠ¨
            )

    with tab4:
        st.subheader("çº¢é»‘æ¦œ (Red/Black List)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»†åˆ†ç»´åº¦çš„åˆ—
        granular_cols = ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·'] 
        if 'å¹¿å‘Šç»„id' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„id')
        if 'å¹¿å‘Šç³»åˆ—' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç³»åˆ—')
        if 'å¹¿å‘Šç»„' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„')
            
        # èšåˆæ•°æ®
        granular_perf = final_df.groupby(granular_cols).agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        granular_perf['ROAS'] = granular_perf.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)
        
        # æ•´ç†åˆ—é¡ºåº
        display_order = [c for c in ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·', 'å¹¿å‘Šç³»åˆ—', 'å¹¿å‘Šç»„', 'å¹¿å‘Šç»„id', 'è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'] if c in granular_perf.columns]
        granular_perf = granular_perf[display_order]

        # æ”¹ä¸ºå‚ç›´æ’åˆ— (Vertical Layout)
        st.markdown("### ğŸ”´ äºæŸæ¦œ (æŒ‰æ¶ˆè€—é™åº - æ­¢æŸä¼˜å…ˆçº§)")
        st.markdown("âš ï¸ **æ­¢æŸå»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹æ¶ˆè€—é«˜ä¸” ROAS < 1.7ï¼Œåº”ä¼˜å…ˆæ£€æŸ¥ç´ ææˆ–å…³åœã€‚")
        
        # çº¢è‰²åˆ—è¡¨æŒ‰è´¹ç”¨é™åºæ’ï¼ˆäºæŸæœ€å¤šçš„æœ€å…ˆçœ‹ï¼‰
        red_list = granular_perf[(granular_perf['ROAS'] < 1.7) & (granular_perf['è´¹ç”¨'] > 0)].sort_values('è´¹ç”¨', ascending=False).head(20)
        st.dataframe(
            red_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                          .background_gradient(subset=['è´¹ç”¨'], cmap="Reds"),
            use_container_width=True,
            hide_index=True
        )
        
        st.markdown("---") # Divider
        
        st.markdown("### ğŸŒŸ æ˜æ˜Ÿæ¦œ (æŒ‰ ROAS é™åº - æ‰©é‡ä¼˜å…ˆçº§)")
        st.markdown("ğŸ’¡ **æ‰©é‡å»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹ ROAS > 2.0ï¼Œæ•ˆç‡æé«˜ï¼Œå¯åœ¨ä¿æŒç¨³å®šçš„å‰æä¸‹é€‚åº¦æ‰©é‡ã€‚")
        
        # é»‘è‰²/æ˜æ˜Ÿåˆ—è¡¨æŒ‰ ROAS é™åºæ’ï¼ˆæ•ˆç‡æœ€é«˜çš„æœ€å…ˆçœ‹ï¼‰
        black_list = granular_perf[granular_perf['ROAS'] > 2.0].sort_values('ROAS', ascending=False).head(20)
        st.dataframe(
            black_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                            .background_gradient(subset=['ROAS'], cmap="Greens"),
            use_container_width=True,
            hide_index=True
        )

    with tab5:
        st.subheader("æ•°æ®ä»“åº“ (Data Warehouse)")
        st.dataframe(final_df)

    with tab6:

        st.subheader("ä¼˜åŒ–å¸ˆç›®æ ‡ç®¡ç† (Manager Goals)")
        
        # 1. å®šä¹‰æ–‡ä»¶è·¯å¾„ (ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä»¥é€‚é…éƒ¨ç½²ç¯å¢ƒ)
        GOAL_CSV_PATH = "Ads_BI/ä¼˜åŒ–å¸ˆè´¦å·ç»´åº¦ç›®æ ‡.csv"
        
        # 0. æ—¥æœŸç­›é€‰ (Date Filter)
        import datetime
        from calendar import monthrange
        col_d, _ = st.columns([1, 3])
        with col_d:
            # ç”¨æˆ·é€‰æ‹©æ—¥æœŸï¼Œæˆ‘ä»¬å°†ä»¥è¯¥æ—¥æœŸæ‰€åœ¨çš„æœˆä»½ä½œä¸ºç»Ÿè®¡å‘¨æœŸ
            target_date = st.date_input("ğŸ“… é€‰æ‹©æˆªæ­¢æ—¥æœŸ (é»˜è®¤ä»Šå¤©)", value=datetime.date.today(), help="å°†ç»Ÿè®¡è¯¥æœˆ1å·åˆ°æ­¤æ—¥æœŸçš„ç´¯è®¡æ•°æ®")
        
        # è®¡ç®—æœˆä»½èµ·æ­¢
        month_start = target_date.replace(day=1)
        filter_end_date = target_date # æˆªæ­¢åˆ°é€‰å®šçš„è¿™ä¸€å¤©
        

        
        _, days_in_month = monthrange(target_date.year, target_date.month)
        
        # è®¡ç®—æ—¶é—´è¿›åº¦
        # é€»è¾‘ä¿®æ”¹ï¼šä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·é€‰æ‹©çš„æˆªæ­¢æ—¥æœŸè®¡ç®—è¿›åº¦
        time_progress = target_date.day / days_in_month

        today = datetime.date.today()
        if target_date == today:
            status_label = "æœ¬æœˆè¿›è¡Œä¸­"
        elif target_date > today:
            status_label = "æœªæ¥é¢„æµ‹"
        else:
            status_label = "å†å²å›æº¯"
            
        st.info(f"ğŸ—“ ç»Ÿè®¡èŒƒå›´: {month_start} ~ {filter_end_date} ({status_label}) | â³ æœˆæ—¶é—´è¿›åº¦: {target_date.day}/{days_in_month} = **{time_progress:.2%}**")

        # 2. è¯»å–ç›®æ ‡æ•°æ®
        if not os.path.exists(GOAL_CSV_PATH):
            st.error(f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {GOAL_CSV_PATH}")
        else:
            try:
                # è¯»å– CSVï¼Œå¤„ç†åƒåˆ†ä½
                goal_df = pd.read_csv(GOAL_CSV_PATH, thousands=',')
                # æ¸…æ´—åˆ—å
                goal_df.columns = [c.strip() for c in goal_df.columns]
                
                # ç¡®ä¿æ•°å€¼åˆ—ä¸ºæµ®ç‚¹æ•°
                for col in ['ç›®æ ‡ROI', 'ç›®æ ‡GMV', 'ç›®æ ‡æ¶ˆè€—é¢']:
                    if col in goal_df.columns:
                        goal_df[col] = pd.to_numeric(goal_df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
                        
                # è¿‡æ»¤æ‰ç©ºè¡Œ
                goal_df = goal_df.dropna(subset=['å¹¿å‘Šè´¦å·'])
                goal_df['å¹¿å‘Šè´¦å·_join'] = goal_df['å¹¿å‘Šè´¦å·'].astype(str).str.strip()

                # --- 3. è·å–åŠç­›é€‰å®é™…æ•°æ® (Actuals) ---
                # ä½¿ç”¨å…¨å±€ df è¿›è¡Œç­›é€‰
                # Note: `df` comes from global scope
                mask_month = (df['å¤©'].dt.date >= month_start) & (df['å¤©'].dt.date <= filter_end_date)
                month_df = df[mask_month].copy()


                
                # èšåˆå®é™…æ•°æ®
                month_agg = month_df.groupby('å¹¿å‘Šè´¦å·').agg({
                    'è´¹ç”¨': 'sum',
                    'è½¬åŒ–ä»·å€¼': 'sum'
                }).reset_index()
                month_agg.rename(columns={'è´¹ç”¨': 'ç´¯è®¡å®é™…æ¶ˆè€—', 'è½¬åŒ–ä»·å€¼': 'ç´¯è®¡GMV'}, inplace=True)
                month_agg['å¹¿å‘Šè´¦å·_join'] = month_agg['å¹¿å‘Šè´¦å·'].astype(str).str.strip()
                
                # --- 4. åˆå¹¶ç›®æ ‡ä¸å®é™… ---
                # Left join goal_df on actuals
                merged = pd.merge(goal_df, month_agg[['å¹¿å‘Šè´¦å·_join', 'ç´¯è®¡å®é™…æ¶ˆè€—', 'ç´¯è®¡GMV']], on='å¹¿å‘Šè´¦å·_join', how='left')
                merged['ç´¯è®¡å®é™…æ¶ˆè€—'] = merged['ç´¯è®¡å®é™…æ¶ˆè€—'].fillna(0)
                merged['ç´¯è®¡GMV'] = merged['ç´¯è®¡GMV'].fillna(0)
                
                # --- 5. è®¡ç®—è¡ç”ŸæŒ‡æ ‡ ---
                # A. æœˆæ—¶é—´è¿›åº¦
                merged['æœˆæ—¶é—´è¿›åº¦'] = time_progress
                
                # B. GMV è¿›åº¦ = ç´¯è®¡GMV / ç›®æ ‡GMV
                merged['GMVè¿›åº¦'] = merged.apply(lambda x: x['ç´¯è®¡GMV'] / x['ç›®æ ‡GMV'] if x['ç›®æ ‡GMV'] > 0 else 0, axis=1)
                
                # C. GMV è¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·
                merged['GMVè¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·'] = merged['GMVè¿›åº¦'] - merged['æœˆæ—¶é—´è¿›åº¦']
                
                # D. æ¶ˆè€—è¿›åº¦ = ç´¯è®¡å®é™…æ¶ˆè€— / ç›®æ ‡æ¶ˆè€—é¢
                merged['æ¶ˆè€—è¿›åº¦'] = merged.apply(lambda x: x['ç´¯è®¡å®é™…æ¶ˆè€—'] / x['ç›®æ ‡æ¶ˆè€—é¢'] if x['ç›®æ ‡æ¶ˆè€—é¢'] > 0 else 0, axis=1)
                
                # E. æ¶ˆè€—åå·®å€¼ = (ç´¯è®¡GMV / ç›®æ ‡ROI) - ç´¯è®¡å®é™…æ¶ˆè€—
                # é€»è¾‘æ¨å¯¼ï¼šæ ¹æ®å›¾ç‰‡æ•°æ® (18159 / 1.9 - 9291 = 266.36 -> 267)
                # å«ä¹‰ï¼šæŒ‰ç…§å®é™…äº§å‡º(GMV)å’Œç›®æ ‡ROIè®¡ç®—å‡ºçš„â€œç†è®ºä¸Šé™æ¶ˆè€—â€ - â€œå®é™…æ¶ˆè€—â€
                # æ­£å€¼ (Green)ï¼šå®é™…èŠ±è´¹ < ç†è®ºä¸Šé™ (çœé¢„ç®—/é«˜ROI)
                # è´Ÿå€¼ (Red)ï¼šå®é™…èŠ±è´¹ > ç†è®ºä¸Šé™ (è¶…æ”¯/ä½ROI)
                merged['æ¶ˆè€—åå·®å€¼'] = merged.apply(lambda x: (x['ç´¯è®¡GMV'] / x['ç›®æ ‡ROI']) - x['ç´¯è®¡å®é™…æ¶ˆè€—'] if x['ç›®æ ‡ROI'] > 0 else -x['ç´¯è®¡å®é™…æ¶ˆè€—'], axis=1)
                
                # F. æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·® = æ¶ˆè€—è¿›åº¦ - GMVè¿›åº¦
                merged['æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·®'] = merged['æ¶ˆè€—è¿›åº¦'] - merged['GMVè¿›åº¦']
                
                # G. è´¦å·çŠ¶æ€è‡ªåŠ¨åŒ–å…¬å¼
                # G. è´¦å·çŠ¶æ€è‡ªåŠ¨åŒ–å…¬å¼
                def get_status(row):
                    if row['ç›®æ ‡æ¶ˆè€—é¢'] == 0:
                        return "æ— è®¡åˆ’æ¶ˆè€—"
                    # Example logic:
                    if row['æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·®'] > 0.10: # Spend > GMV by 10%
                        return "æ¶ˆè€—è¿‡å¿« (éœ€ä¼˜åŒ–)"
                    elif row['GMVè¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·'] < -0.20:
                         return "è¿›åº¦ä¸¥é‡æ»å"
                    return "æ­£å¸¸ (æ— éœ€å¹²é¢„)"

                merged['è´¦å·çŠ¶æ€'] = merged.apply(get_status, axis=1)

                # 6. æ„é€ æœ€ç»ˆå±•ç¤º DataFrame
                display_cols = [
                    'ä¼˜åŒ–å¸ˆ', 'å¹¿å‘Šè´¦å·', 
                    'ç›®æ ‡ROI', 'æœˆæ—¶é—´è¿›åº¦', 
                    'ç›®æ ‡GMV', 'ç´¯è®¡GMV', 'GMVè¿›åº¦', 'GMVè¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·',
                    'ç›®æ ‡æ¶ˆè€—é¢', 'ç´¯è®¡å®é™…æ¶ˆè€—', 'æ¶ˆè€—è¿›åº¦', 'æ¶ˆè€—åå·®å€¼', 
                    'æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·®', 'è´¦å·çŠ¶æ€'
                ]
                # é‡å‘½åä»¥ä¾¿å±•ç¤º 'ç›®æ ‡æ¶ˆè€—é¢' -> 'ç›®æ ‡æ¶ˆè€—'
                rename_map = {'ç›®æ ‡æ¶ˆè€—é¢': 'ç›®æ ‡æ¶ˆè€—'}
                final_view = merged[display_cols].rename(columns=rename_map).copy()

                # --- Aggregation Row (åˆè®¡) ---
                sum_row = final_view.sum(numeric_only=True)
                sum_row['ä¼˜åŒ–å¸ˆ'] = 'åˆè®¡'
                sum_row['å¹¿å‘Šè´¦å·'] = ''
                sum_row['è´¦å·çŠ¶æ€'] = ''
                sum_row['æœˆæ—¶é—´è¿›åº¦'] = time_progress
                
                # Re-calc ratios for Total
                total_gmv = sum_row['ç´¯è®¡GMV']
                total_goal_gmv = sum_row['ç›®æ ‡GMV']
                total_spend = sum_row['ç´¯è®¡å®é™…æ¶ˆè€—']
                total_goal_spend = sum_row['ç›®æ ‡æ¶ˆè€—']
                
                sum_row['GMVè¿›åº¦'] = total_gmv / total_goal_gmv if total_goal_gmv > 0 else 0
                sum_row['GMVè¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·'] = sum_row['GMVè¿›åº¦'] - time_progress
                sum_row['æ¶ˆè€—è¿›åº¦'] = total_spend / total_goal_spend if total_goal_spend > 0 else 0
                # sum_row['æ¶ˆè€—åå·®å€¼'] ä¸éœ€è¦é‡ç®—ï¼Œç›´æ¥ç´¯åŠ å³å¯åæ˜ æ•´ä½“ç›ˆäº
                # sum_row['æ¶ˆè€—åå·®å€¼'] = sum_row['ç´¯è®¡å®é™…æ¶ˆè€—'] - (sum_row['ç›®æ ‡æ¶ˆè€—'] * time_progress) # DELETE OLD
                
                sum_row['æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·®'] = sum_row['æ¶ˆè€—è¿›åº¦'] - sum_row['GMVè¿›åº¦']
                
                # Weighted ROI
                if total_spend > 0:
                    # ROI = Total GMV / Total Spend ?? Or Avg Target ROI?
                    # Usually "Target ROI" for Total is Goal GMV / Goal Spend
                    sum_row['ç›®æ ‡ROI'] = total_goal_gmv / total_goal_spend if total_goal_spend > 0 else 0
                else:
                    sum_row['ç›®æ ‡ROI'] = 0

                final_view = pd.concat([final_view, pd.DataFrame([sum_row])], ignore_index=True)
                
                # --- 7. Styling ---
                styler = final_view.style.format({
                    'ç›®æ ‡ROI': "{:.2f}",
                    'æœˆæ—¶é—´è¿›åº¦': "{:.2%}",
                    'ç›®æ ‡GMV': "{:,.0f}",
                    'ç´¯è®¡GMV': "{:,.0f}",
                    'GMVè¿›åº¦': "{:.2%}",
                    'GMVè¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·': "{:.2%}",
                    'ç›®æ ‡æ¶ˆè€—': "{:,.0f}",
                    'ç´¯è®¡å®é™…æ¶ˆè€—': "{:,.0f}",
                    'æ¶ˆè€—è¿›åº¦': "{:.2%}",
                    'æ¶ˆè€—åå·®å€¼': "{:,.0f}",
                    'æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·®': "{:.2%}"
                })
                
                # Color Logics
                def color_gmv_diff(v):
                    if pd.isna(v): return ''
                    return 'color: red; font-weight: bold' if v < 0 else ''
                
                def color_spend_gmv_diff(v):
                    if pd.isna(v): return ''
                    if v > 0: return 'color: red' # Spend faster than GMV -> Inefficient
                    if v < 0: return 'color: green'
                    return ''
                    
                def color_deviation(v):
                    if pd.isna(v): return ''
                    return 'color: red' if v < 0 else '' # Underspend logic
                
                def color_status(v):
                    return 'color: red' if v == 'æ— è®¡åˆ’æ¶ˆè€—' else ''

                styler.map(color_gmv_diff, subset=['GMVè¿›åº¦ä¸æ—¶é—´è¿›åº¦å·®è·'])
                styler.map(color_spend_gmv_diff, subset=['æ¶ˆè€—è¿›åº¦ä¸GMVè¿›åº¦å·®'])
                styler.map(color_deviation, subset=['æ¶ˆè€—åå·®å€¼'])
                styler.map(color_status, subset=['è´¦å·çŠ¶æ€'])

                st.dataframe(styler, use_container_width=True, height=600)
                
            except Exception as e:
                st.error(f"å¤„ç†ç›®æ ‡æ–‡ä»¶å‡ºé”™: {e}")



if __name__ == "__main__":
    main()

