import streamlit as st
import pandas as pd
import plotly.express as px
import gspread
from google.oauth2.service_account import Credentials
from urllib.parse import urlparse
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re

# -----------------------------------------------------------------------------
# é…ç½®ä¸è¯´æ˜
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="Antigravity Ads Cloud - å¹¿å‘Šäº‘",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¤è¯é…ç½®
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

# -----------------------------------------------------------------------------
# æ•°æ®åŠ è½½ä¸å¤„ç† (ETL)
# -----------------------------------------------------------------------------

def get_gspread_client():
    """ä½¿ç”¨ Streamlit secrets è¿›è¡Œ Google Sheets è®¤è¯"""
    try:
        creds_dict = dict(st.secrets["connections"]["gsheets"])
        creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        client = gspread.authorize(creds)
        return client
    except Exception as e:
        st.error(f"è®¤è¯é”™è¯¯: {e}")
        st.stop()

@st.cache_data(ttl=600)  # ç¼“å­˜ 10 åˆ†é’Ÿ
def load_data():
    """
    ä» Google Sheets åŠ è½½æ•°æ®å¹¶æ‰§è¡ŒåŒé”®æ˜ å°„ã€‚
    """
    client = get_gspread_client()
    spreadsheet_url = st.secrets["connections"]["gsheets"]["spreadsheet"]
    
    try:
        if spreadsheet_url.startswith("http"):
            sh = client.open_by_url(spreadsheet_url)
        else:
            sh = client.open(spreadsheet_url)
            
        def read_sheet(worksheet_name, cols=None):
            ws = sh.worksheet(worksheet_name)
            data = ws.get_all_values()
            if not data:
                return pd.DataFrame()
            
            # å»é™¤è¡¨å¤´å‰åç©ºæ ¼
            headers = [h.strip() for h in data[0]]
            rows = data[1:]
            
            df = pd.DataFrame(rows, columns=headers)
            
            # å…³é”®ä¿®å¤ï¼šå»é™¤é‡å¤åˆ—åï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
            df = df.loc[:, ~df.columns.duplicated()]
            
            # å»é™¤ç©ºè¡¨å¤´åˆ—
            df = df.loc[:, df.columns != '']
            
            if cols:
                # åªæœ‰å½“åˆ—å­˜åœ¨æ—¶æ‰ç­›é€‰
                existing_cols = [c for c in cols if c in df.columns]
                if existing_cols:
                    df = df[existing_cols]
            return df
            
        raw_df = read_sheet("Raw_Data")
        manager_map = read_sheet("Manager_Map")
        category_map = read_sheet("Category_Map")

        if raw_df.empty:
            return pd.DataFrame()

        # ---------------------------------------------------------------------
        # èƒ½å¤Ÿè¯†åˆ«çš„åˆ—åæ˜ å°„ (Robust Column Mapping)
        # ---------------------------------------------------------------------
        column_mapping = {
            'è½¬åŒ–ä»·å€¼ (æŒ‰è½¬åŒ–æ—¶é—´)': 'è½¬åŒ–ä»·å€¼',
            'è½¬åŒ–ä»·å€¼ (æŒ‰è½¬åŒ–æ—¶é—´) ': 'è½¬åŒ–ä»·å€¼', # Handle trailing space
            'è½¬åŒ–ä»·å€¼ï¼ˆæŒ‰è½¬åŒ–æ—¶é—´ï¼‰': 'è½¬åŒ–ä»·å€¼', # Chinese parenthesis
            'è½¬åŒ–ä»·å€¼ï¼ˆæŒ‰è½¬åŒ–æ—¶é—´ï¼‰ ': 'è½¬åŒ–ä»·å€¼',
            'Cost': 'è´¹ç”¨',
            'Conversions': 'è½¬åŒ–æ•°',
            'Conversion value': 'è½¬åŒ–ä»·å€¼'
            # Add other known aliases here
        }
        raw_df = raw_df.rename(columns=column_mapping)
        # å…³é”®ä¿®å¤ï¼šé‡å‘½ååå¯èƒ½äº§ç”Ÿé‡å¤åˆ—ï¼ˆä¾‹å¦‚åŒæ—¶æœ‰åä¸º A å’Œ B çš„åˆ—ï¼Œéƒ½è¢«æ˜ å°„ä¸º Cï¼‰ï¼Œå†æ¬¡å»é‡
        raw_df = raw_df.loc[:, ~raw_df.columns.duplicated()]
        # ---------------------------------------------------------------------

        # åŸºç¡€æ¸…æ´—
        if 'å¤©' in raw_df.columns:
            raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'], errors='coerce')
        # åŸºç¡€æ¸…æ´—
        if 'å¤©' in raw_df.columns:
            raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'], errors='coerce')
        if 'å¹¿å‘Šè´¦å·' in raw_df.columns:
            # ç”¨æˆ·åé¦ˆ Raw Data ä¸­çš„ ID å¯èƒ½åŒ…å«åç¼€ (e.g. "ID | filename")ï¼Œä¸” split('|') å¯èƒ½å¤±æ•ˆ
            # æ”¹ç”¨æ­£åˆ™æå–æ ‡å‡†çš„ Google Ads ID æ ¼å¼ (xxx-xxx-xxxx)
            extracted_ids = raw_df['å¹¿å‘Šè´¦å·'].astype(str).str.extract(r'(\d{3}-\d{3}-\d{4})', expand=False)
            # å¦‚æœæå–åˆ°äº†å°±ç”¨æå–çš„ï¼Œæ²¡æå–åˆ°ï¼ˆå¯èƒ½æ˜¯çº¯æ•°å­—æˆ–å…¶ä»–æ ¼å¼ï¼‰å°±ä¿ç•™åŸæ ·ä½†å»é™¤ç©ºæ ¼
            raw_df['å¹¿å‘Šè´¦å·'] = extracted_ids.fillna(raw_df['å¹¿å‘Šè´¦å·'].astype(str)).str.strip()
            
            # å…³é”®ä¿®å¤ï¼šè¿‡æ»¤æ‰â€œå¹¿å‘Šè´¦å·â€ä¸ºç©ºçš„è¡Œ (ä¾‹å¦‚ Google Sheet çš„ç©ºè¡Œ)
            raw_df = raw_df[raw_df['å¹¿å‘Šè´¦å·'] != '']
        
        if 'å¹¿å‘Šè´¦å·' in manager_map.columns:
            manager_map['å¹¿å‘Šè´¦å·'] = manager_map['å¹¿å‘Šè´¦å·'].astype(str).str.strip() # å»é™¤ ID ç©ºæ ¼

        # 2. æ˜ å°„ä¼˜åŒ–å¸ˆ
        if 'å¹¿å‘Šè´¦å·' in raw_df.columns and 'å¹¿å‘Šè´¦å·' in manager_map.columns:
            merged_df = pd.merge(raw_df, manager_map, on='å¹¿å‘Šè´¦å·', how='left')
        else:
            merged_df = raw_df.copy()
            merged_df['ä¼˜åŒ–å¸ˆ'] = "Unknown"

        merged_df['ä¼˜åŒ–å¸ˆ'] = merged_df.get('ä¼˜åŒ–å¸ˆ', pd.Series(["Unknown"]*len(merged_df))).fillna("Unknown")
        
        # --- DEBUG: æ˜ å°„è¯Šæ–­ ---
        # å¦‚æœå¤§é‡ Unknownï¼Œç»™ç”¨æˆ·å±•ç¤ºåŸå› 
        unknown_count = (merged_df['ä¼˜åŒ–å¸ˆ'] == 'Unknown').sum()
        if unknown_count > 5:
            with st.expander(f"âš ï¸ å‘ç° {unknown_count} æ¡æ•°æ®æœªåŒ¹é…åˆ°ä¼˜åŒ–å¸ˆ (ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…)"):
                st.write("Manager Map (å‰ 5 è¡Œ):")
                st.dataframe(manager_map.head())
                st.write("Raw Data ä¸­æœªåŒ¹é…çš„å¹¿å‘Šè´¦å· (å‰ 10 ä¸ª):")
                unmatched_ids = merged_df[merged_df['ä¼˜åŒ–å¸ˆ'] == 'Unknown']['å¹¿å‘Šè´¦å·'].unique()
                st.write(unmatched_ids[:10])
                st.info("è¯·æ£€æŸ¥ 'Ad Account' (Raw_Data) å’Œ 'å¹¿å‘Šè´¦å·' (Manager_Map) æ˜¯å¦ä¸€è‡´ã€‚")
        # ---------------------

        # 3. æ˜ å°„ç±»ç›®
        def clean_url(url):
            if pd.isna(url):
                return ""
            parsed = urlparse(str(url))
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')

        if 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in merged_df.columns:
             merged_df['clean_url'] = merged_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url)
        else:
             merged_df['clean_url'] = ""

        if 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in category_map.columns:
            category_map['clean_url'] = category_map['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url)
        else:
             category_map['clean_url'] = "" # Should define clean_url column anyway
        
        if 'ç±»ç›®' in category_map.columns:
            category_map_dedup = category_map[['clean_url', 'ç±»ç›®']].drop_duplicates()
            merged_df = pd.merge(merged_df, category_map_dedup, on='clean_url', how='left')
            merged_df['ç±»ç›®'] = merged_df['ç±»ç›®'].fillna("Unknown")
        else:
            merged_df['ç±»ç›®'] = "Unknown"
            
        if 'clean_url' in merged_df.columns:
            merged_df = merged_df.drop(columns=['clean_url'])
        
        numeric_cols = ['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS', 'è½¬åŒ–æ•°']
        missing_numeric = [col for col in numeric_cols if col not in merged_df.columns and col != 'è½¬åŒ–æ•°'] # è½¬åŒ–æ•° is optional
        
        if missing_numeric:
            st.error(f"ä¸¥é‡é”™è¯¯ï¼šæ‚¨çš„ Google Sheet 'Raw_Data' è¡¨ç¼ºå°‘ä»¥ä¸‹å…³é”®æ•°æ®åˆ—: {missing_numeric}ã€‚")
            st.write(f"å½“å‰æ£€æµ‹åˆ°çš„æ‰€æœ‰åˆ—å: {list(merged_df.columns)}")
            st.info("è¯·æ£€æŸ¥æ‚¨çš„è¡¨æ ¼è¡¨å¤´æ˜¯å¦æœ‰é”™åˆ«å­—ã€å¤šä½™ç©ºæ ¼ï¼Œæˆ–è€…åˆ—åä¸åŒ¹é…ã€‚")
            # Return empty to prevent KeyError downstream
            return pd.DataFrame()

        for col in numeric_cols:
             if col in merged_df.columns:
                merged_df[col] = merged_df[col].astype(str).str.replace(r'[^\d\.-]', '', regex=True)
                merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce').fillna(0)
        
        # å¼ºåˆ¶é‡æ–°è®¡ç®— ROASï¼Œç¡®ä¿æ•°æ®å‡†ç¡® (å³ä½¿ Sheet é‡Œæœ‰è¿™åˆ—ä½†ä¸ºç©º)
        if 'è´¹ç”¨' in merged_df.columns and 'è½¬åŒ–ä»·å€¼' in merged_df.columns:
             merged_df['ROAS'] = merged_df.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)

        return merged_df

    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return pd.DataFrame()

# -----------------------------------------------------------------------------
# æ•°æ®ä¸Šä¼ é€»è¾‘
# -----------------------------------------------------------------------------

def upload_data(uploaded_files):
    if not uploaded_files:
        return
    
    client = get_gspread_client()
    spreadsheet_url = st.secrets["connections"]["gsheets"]["spreadsheet"]
    
    if spreadsheet_url.startswith("http"):
        sh = client.open_by_url(spreadsheet_url)
    else:
        sh = client.open(spreadsheet_url)
    
    ws = sh.worksheet("Raw_Data")
    
    try:
        data = ws.get_all_values()
        if data:
            headers = [h.strip() for h in data[0]]
            existing_records = data[1:]
            if existing_records:
                current_df = pd.DataFrame(existing_records, columns=headers)
                current_df = current_df.loc[:, current_df.columns != '']
            else:
                 current_df = pd.DataFrame(columns=headers)
        else:
            current_df = pd.DataFrame()
    except Exception as e:
        st.error(f"è¯»å–ç°æœ‰æ•°æ®å‡ºé”™: {e}")
        current_df = pd.DataFrame()

    if not current_df.empty:
        required_cols = ['å¤©', 'å¹¿å‘Šè´¦å·']
        if not all(col in current_df.columns for col in required_cols):
             st.warning(f"è­¦å‘Šï¼šæ‚¨çš„ Google Sheet 'Raw_Data' å·¥ä½œè¡¨ç¼ºå°‘å¿…è¦çš„åˆ—å¤´: {required_cols}ã€‚è¯·ç¡®ä¿è¡¨å¤´ä¸º: å¤©, å¹¿å‘Šè´¦å·, æœ€ç»ˆåˆ°è¾¾ç½‘å€, è´¹ç”¨, è½¬åŒ–ä»·å€¼, è½¬åŒ–æ•° (å¯é€‰)")
             # è™½ç„¶ç¼ºå°‘åˆ—ï¼Œä½†å¦‚æœç”¨æˆ·ç¡®è®¤è¦ä¼ ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ã€‚ä½†å»é‡ä¼šå¤±æ•ˆã€‚
             # ä¸ºäº†å®‰å…¨ï¼Œè¿™é‡Œæˆ‘ä»¬ä»ç„¶å…è®¸ä¸Šä¼ ï¼Œä½†å»é‡é€»è¾‘ä¼šè¢«è·³è¿‡ã€‚
             existing_keys = set()
        else:
            current_df['å¤©'] = pd.to_datetime(current_df['å¤©'], errors='coerce')
            current_df['å¹¿å‘Šè´¦å·'] = current_df['å¹¿å‘Šè´¦å·'].astype(str)
            existing_keys = set(zip(current_df['å¹¿å‘Šè´¦å·'], current_df['å¤©']))
    else:
        existing_keys = set()
        # å¦‚æœå½“å‰ sheet ä¸ºç©ºï¼Œæˆ‘ä»¬éœ€è¦è¿™ä¸€æ­¥çš„ headers å—ï¼Ÿ
        # å¦‚æœ data ä¸ºç©ºï¼Œws.get_all_values() è¿”å› []ï¼Œæ‰€ä»¥ headers æœªå®šä¹‰ã€‚
        # æˆ‘ä»¬éœ€è¦åœ¨ new_rows_list ç”Ÿæˆåï¼Œæ ¹æ® new_rows key æ¥ä½œä¸º header åˆå§‹åŒ– sheetã€‚
        pass

    # åŠ è½½ç±»ç›®æ˜ å°„ä»¥ç”Ÿæˆ å¹¿å‘Šç»„id
    try:
        def clean_url_local(url):
            if pd.isna(url) or not url: return ""
            parsed = urlparse(str(url))
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')
            
        cat_ws = sh.worksheet("Category_Map")
        cat_data = cat_ws.get_all_values()
        if cat_data:
            cat_headers = [h.strip() for h in cat_data[0]]
            cat_df = pd.DataFrame(cat_data[1:], columns=cat_headers)
            cat_df['clean_url'] = cat_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url_local)
            cat_lookup = cat_df.set_index('clean_url')['ç±»ç›®'].to_dict()
        else:
            cat_lookup = {}
    except:
        cat_lookup = {}

    new_rows_list = []
    
    for uploaded_file in uploaded_files:
        try:
            filename = uploaded_file.name
            # ä½¿ç”¨æ­£åˆ™æå–è´¦å· ID (xxx-xxx-xxxx)
            match = re.search(r'(\d{3}-\d{3}-\d{4})', filename)
            account_id = match.group(0) if match else filename.split('.')[0].strip()
            
            # å°è¯•å¤šç§ç¼–ç å’Œåˆ†éš”ç¬¦è¯»å– CSV
            df = None
            last_err = ""
            for enc in ['utf-8-sig', 'utf-16', 'utf-8', 'gbk']:
                try:
                    uploaded_file.seek(0)
                    # ä½¿ç”¨ sep=None å’Œ engine='python' è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦ (å¦‚é€—å·ã€åˆ¶è¡¨ç¬¦)
                    temp_df = pd.read_csv(uploaded_file, encoding=enc, sep=None, engine='python')
                    if not temp_df.empty and len(temp_df.columns) >= 2:
                        df = temp_df
                        break
                except Exception as e:
                    last_err = str(e)
                    continue
            
            if df is None:
                st.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}ã€‚é”™è¯¯è¯¦æƒ…: {last_err}")
                st.info("å»ºè®®ï¼šè¯·å°è¯•åœ¨ Excel ä¸­æ‰“å¼€è¯¥æ–‡ä»¶ï¼Œæ£€æŸ¥å†…å®¹æ˜¯å¦æ­£å¸¸ï¼Œå¹¶å¦å­˜ä¸ºæ ‡å‡†çš„ CSV (é€—å·åˆ†éš”) æ ¼å¼åé‡æ–°ä¸Šä¼ ã€‚")
                continue
            
            df['å¹¿å‘Šè´¦å·'] = account_id
            # å…¼å®¹è¡¨å¤´ä¸­å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼
            df.columns = [c.strip() for c in df.columns]
            
            df['å¤©'] = pd.to_datetime(df['å¤©'], errors='coerce')
            df = df.dropna(subset=['å¤©']) # è¿‡æ»¤æ‰æ—¥æœŸè§£æå¤±è´¥çš„è¡Œ
            
            for _, row in df.iterrows():
                key = (str(row['å¹¿å‘Šè´¦å·']), row['å¤©'])
                if key not in existing_keys:
                    row_dict = row.to_dict()
                    row_dict['å¤©'] = row['å¤©'].strftime('%Y-%m-%d')
                    row_dict['å¹¿å‘Šè´¦å·'] = str(row_dict['å¹¿å‘Šè´¦å·'])
                    
                    # åŒ¹é…ç±»ç›®
                    url_val = str(row_dict.get('æœ€ç»ˆåˆ°è¾¾ç½‘å€', ''))
                    clean_u = clean_url_local(url_val)
                    category_found = cat_lookup.get(clean_u, "Unknown")
                    row_dict['ç±»ç›®'] = category_found
                    
                    # ç”Ÿæˆå¹¿å‘Šç»„id: å¹¿å‘Šè´¦å· + å¹¿å‘Šç³»åˆ— + ç±»ç›® + è½åœ°é¡µ + å¹¿å‘Šç»„
                    acc = str(row_dict.get('å¹¿å‘Šè´¦å·', ''))
                    cmp = str(row_dict.get('å¹¿å‘Šç³»åˆ—', ''))
                    cat = str(row_dict.get('ç±»ç›®', ''))
                    grp = str(row_dict.get('å¹¿å‘Šç»„', ''))
                    
                    row_dict['å¹¿å‘Šç»„id'] = f"{acc}{cmp}{cat}{url_val}{grp}"
                    
                    new_rows_list.append(row_dict)
                    
        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {e}")
    
    if new_rows_list:
        # è·å–æœ€æ–°çš„ headers (å¦‚æœ sheet ä¸ä¸ºç©º)
        if not current_df.empty and 'headers' in locals():
            target_headers = headers
        elif 'headers' in locals() and headers: # sheet æœ‰å¤´ä½†æ— æ•°æ®
             target_headers = headers
        else:
            # Sheet æ˜¯å®Œå…¨ç©ºçš„ï¼Œç”¨æ–°æ•°æ®çš„ keys ä½œä¸º headers
            target_headers = list(new_rows_list[0].keys())
            # ç¡®ä¿å…³é”®åˆ—åœ¨å‰é¢? å¯é€‰ã€‚å…ˆè¿™æ ·ã€‚
            ws.append_row(target_headers) # å…ˆå†™å…¥è¡¨å¤´
        
        rows_to_append = []
        for item in new_rows_list:
            row_data = []
            for col in target_headers:
                val = item.get(col, "")
                if pd.isna(val):
                     val = ""
                row_data.append(val)
            rows_to_append.append(row_data)
            
        ws.append_rows(rows_to_append)
        
        st.success(f"æˆåŠŸæ·»åŠ  {len(new_rows_list)} è¡Œæ•°æ®ï¼")
        st.cache_data.clear()
    else:
        st.warning("æ²¡æœ‰æ–°æ•°æ®å¯æ·»åŠ ï¼ˆæ£€æµ‹åˆ°é‡å¤æ•°æ®ï¼‰ã€‚")

# -----------------------------------------------------------------------------
# ä¸»åº”ç”¨ç¨‹åº
# -----------------------------------------------------------------------------

def main():
    st.title("Antigravity Ads Cloud ğŸš€ - å¹¿å‘Šäº‘")
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    if df.empty:
        st.warning("æœªæ‰¾åˆ°æ•°æ®æˆ–è¿æ¥å¤±è´¥ã€‚å¯èƒ½æ˜¯ç½‘ç»œæ³¢åŠ¨ï¼Œè¯·å°è¯•åˆ·æ–°ã€‚")
        if st.button("ğŸ”„ é‡è¯•è¿æ¥ (Retry)"):
            st.cache_data.clear()
            st.rerun()
        with st.sidebar:
            st.header("æ•°æ®ä¸Šä¼ ")
            uploaded_files = st.file_uploader("ä¸Šä¼  CSV", accept_multiple_files=True, type="csv")
            if uploaded_files:
                if st.button("å¤„ç†å¹¶ä¸Šä¼ "):
                    upload_data(uploaded_files)
        return

    # -------------------------------------------------------------------------
    # ä¾§è¾¹æ ç­›é€‰å™¨
    # -------------------------------------------------------------------------
    st.sidebar.header("å…¨å±€ç­›é€‰å™¨")
    
    if st.sidebar.button("ğŸ”„ åˆ·æ–°æ•°æ® (é‡ç½®ç¼“å­˜)"):
        st.cache_data.clear()
        st.rerun()
    
    min_date = df['å¤©'].min().date()
    max_date = df['å¤©'].max().date()
    start_date = st.sidebar.date_input("å¼€å§‹æ—¥æœŸ", value=max(min_date, max_date - pd.Timedelta(days=30)))
    end_date = st.sidebar.date_input("ç»“æŸæ—¥æœŸ", value=max_date)

    mask_date = (df['å¤©'].dt.date >= start_date) & (df['å¤©'].dt.date <= end_date)
    df_filtered_date = df[mask_date]

    managers = ["æ•´ä½“"] + sorted(df_filtered_date['ä¼˜åŒ–å¸ˆ'].unique().tolist())
    selected_managers = st.sidebar.multiselect("ä¼˜åŒ–å¸ˆ", managers, default=["æ•´ä½“"])
    
    if "æ•´ä½“" in selected_managers:
        df_filtered_manager = df_filtered_date
    else:
        df_filtered_manager = df_filtered_date[df_filtered_date['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)]
    
    categories = sorted(df_filtered_manager['ç±»ç›®'].unique().tolist())
    selected_categories = st.sidebar.multiselect("ç±»ç›®", categories, default=categories)
    
    df_filtered_category = df_filtered_manager[df_filtered_manager['ç±»ç›®'].isin(selected_categories)]
    
    accounts = sorted(df_filtered_category['å¹¿å‘Šè´¦å·'].unique().tolist())
    selected_accounts = st.sidebar.multiselect("å¹¿å‘Šè´¦å·", accounts, default=accounts)
    
    final_df = df_filtered_category[df_filtered_category['å¹¿å‘Šè´¦å·'].isin(selected_accounts)]

    st.sidebar.markdown("---")
    st.sidebar.header("æ•°æ®ä¸Šä¼ ")
    uploaded_files = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", accept_multiple_files=True, type="csv")
    if uploaded_files:
        if st.button("å¤„ç†å¹¶ä¸Šä¼ "):
            upload_data(uploaded_files)

    # -------------------------------------------------------------------------
    # Tabs
    # -------------------------------------------------------------------------
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "æŒ‡æŒ¥ä¸­å¿ƒ", "å›¢é˜Ÿä¸æˆ˜ç•¥", "æ·±åº¦é€è§†", "çº¢é»‘æ¦œ (å¼‚å¸¸è¯Šæ–­)", "æ•°æ®ä»“åº“"
    ])

    with tab1:
        st.subheader("æŒ‡æŒ¥ä¸­å¿ƒ (Command Center)")
        
        current_spend = final_df['è´¹ç”¨'].sum()
        current_val = final_df['è½¬åŒ–ä»·å€¼'].sum()
        current_roas = current_val / current_spend if current_spend > 0 else 0
        
        has_conversions = 'è½¬åŒ–æ•°' in final_df.columns
        current_conversions = final_df['è½¬åŒ–æ•°'].sum() if has_conversions else 1
        current_cpa = current_spend / current_conversions if has_conversions and current_conversions > 0 else 0

        days_diff = (end_date - start_date).days + 1
        prev_start = start_date - pd.Timedelta(days=days_diff)
        prev_end = start_date - pd.Timedelta(days=1)
        
        mask_prev = (df['å¤©'].dt.date >= prev_start) & (df['å¤©'].dt.date <= prev_end) & \
                    (df['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)) & \
                    (df['ç±»ç›®'].isin(selected_categories)) & \
                    (df['å¹¿å‘Šè´¦å·'].isin(selected_accounts))
        prev_df = df[mask_prev]
        
        prev_spend = prev_df['è´¹ç”¨'].sum()
        prev_val = prev_df['è½¬åŒ–ä»·å€¼'].sum()
        prev_roas = prev_val / prev_spend if prev_spend > 0 else 0
        
        permil_delta_spend = ((current_spend - prev_spend) / prev_spend) * 100 if prev_spend > 0 else 0
        permil_delta_roas = ((current_roas - prev_roas) / prev_roas) * 100 if prev_roas > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        col1.metric("æ€»æ¶ˆè€— (Total Spend)", f"${current_spend:,.2f}", f"{permil_delta_spend:.2f}%")
        col2.metric("æ•´ä½“ ROAS", f"{current_roas:.2f}", f"{permil_delta_roas:.2f}%")
        col3.metric("æ€»è½¬åŒ–ä»·å€¼ (Total Value)", f"${current_val:,.2f}") 

        st.markdown("### ä¸šç»©è¶‹åŠ¿ (Performance Trend)")
        daily_trend = final_df.groupby('å¤©').agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        daily_trend['ROAS'] = daily_trend['è½¬åŒ–ä»·å€¼'] / daily_trend['è´¹ç”¨']
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(
            go.Bar(x=daily_trend['å¤©'], y=daily_trend['è´¹ç”¨'], name="æ¶ˆè€— (Spend)"),
            secondary_y=False,
        )
        fig.add_trace(
            go.Scatter(x=daily_trend['å¤©'], y=daily_trend['ROAS'], name="ROAS", mode='lines+markers'),
            secondary_y=True,
        )
        fig.update_layout(title_text="æ¶ˆè€— vs ROAS è¶‹åŠ¿")
        fig.update_yaxes(title_text="æ¶ˆè€— (Spend)", secondary_y=False)
        fig.update_yaxes(title_text="ROAS", secondary_y=True)
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        st.subheader("å›¢é˜Ÿä¸æˆ˜ç•¥ (Team & Strategy)")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### äººæ•ˆçŸ©é˜µ (People Matrix)")
            manager_perf = final_df.groupby('ä¼˜åŒ–å¸ˆ').agg({
                'è´¹ç”¨': 'sum', 
                'è½¬åŒ–ä»·å€¼': 'sum'
            }).reset_index()
            manager_perf['ROAS'] = manager_perf['è½¬åŒ–ä»·å€¼'] / manager_perf['è´¹ç”¨']
            
            fig_bubble = px.scatter(
                manager_perf, x="è´¹ç”¨", y="ROAS", size="è½¬åŒ–ä»·å€¼", color="ä¼˜åŒ–å¸ˆ",
                hover_name="ä¼˜åŒ–å¸ˆ", title="ä¼˜åŒ–å¸ˆè¡¨ç°çŸ©é˜µ", size_max=60
            )
            fig_bubble.add_hline(y=1.0, line_dash="dash", line_color="red")
            st.plotly_chart(fig_bubble, use_container_width=True)
            
        with col2:
            st.markdown("#### å“ç±»ç‰ˆå›¾ (Category Share)")
            cat_perf = final_df.groupby('ç±»ç›®').agg({'è´¹ç”¨': 'sum'}).reset_index()
            fig_pie = px.pie(cat_perf, values='è´¹ç”¨', names='ç±»ç›®', title="å„å“ç±»æ¶ˆè€—å æ¯”")
            st.plotly_chart(fig_pie, use_container_width=True)

    with tab3:
        st.subheader("æ·±åº¦é€è§† (Deep Pivot)")
        c1, c2 = st.columns(2)
        with c1:
            pivot_rows = st.multiselect("è¡Œç»´åº¦", ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¤©', 'å¹¿å‘Šè´¦å·'], default=['ä¼˜åŒ–å¸ˆ'])
        with c2:
            pivot_vals = st.multiselect("æ•°å€¼æŒ‡æ ‡", ['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'], default=['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'])
            
        if pivot_rows and pivot_vals:
            pivot_df = final_df.groupby(pivot_rows)[['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼']].sum().reset_index()
            pivot_df['ROAS'] = pivot_df['è½¬åŒ–ä»·å€¼'] / pivot_df['è´¹ç”¨']
            display_cols = pivot_rows + pivot_vals
            st.dataframe(
                pivot_df[display_cols].style.background_gradient(subset=['ROAS'], cmap="RdYlGn", vmin=0.5, vmax=2.0),
                use_container_width=True
            )

    with tab4:
        st.subheader("çº¢é»‘æ¦œ (Red/Black List)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»†åˆ†ç»´åº¦çš„åˆ—
        granular_cols = ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·'] 
        if 'å¹¿å‘Šç»„id' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„id')
        if 'å¹¿å‘Šç³»åˆ—' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç³»åˆ—')
        if 'å¹¿å‘Šç»„' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„')
            
        # èšåˆæ•°æ®
        granular_perf = final_df.groupby(granular_cols).agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        granular_perf['ROAS'] = granular_perf.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)
        
        # æ•´ç†åˆ—é¡ºåº
        display_order = [c for c in ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·', 'å¹¿å‘Šç³»åˆ—', 'å¹¿å‘Šç»„', 'å¹¿å‘Šç»„id', 'è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'] if c in granular_perf.columns]
        granular_perf = granular_perf[display_order]

        # æ”¹ä¸ºå‚ç›´æ’åˆ— (Vertical Layout)
        st.markdown("### ğŸ”´ äºæŸæ¦œ (æŒ‰æ¶ˆè€—é™åº - æ­¢æŸä¼˜å…ˆçº§)")
        st.markdown("âš ï¸ **æ­¢æŸå»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹æ¶ˆè€—é«˜ä¸” ROAS < 1.0ï¼Œåº”ä¼˜å…ˆæ£€æŸ¥ç´ ææˆ–å…³åœã€‚")
        
        # çº¢è‰²åˆ—è¡¨æŒ‰è´¹ç”¨é™åºæ’ï¼ˆäºæŸæœ€å¤šçš„æœ€å…ˆçœ‹ï¼‰
        red_list = granular_perf[(granular_perf['ROAS'] < 1.0) & (granular_perf['è´¹ç”¨'] > 0)].sort_values('è´¹ç”¨', ascending=False).head(20)
        st.dataframe(
            red_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                          .background_gradient(subset=['è´¹ç”¨'], cmap="Reds"),
            use_container_width=True,
            hide_index=True
        )
        
        st.markdown("---") # Divider
        
        st.markdown("### ğŸŒŸ æ˜æ˜Ÿæ¦œ (æŒ‰ ROAS é™åº - æ‰©é‡ä¼˜å…ˆçº§)")
        st.markdown("ğŸ’¡ **æ‰©é‡å»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹ ROAS > 2.0ï¼Œæ•ˆç‡æé«˜ï¼Œå¯åœ¨ä¿æŒç¨³å®šçš„å‰æä¸‹é€‚åº¦æ‰©é‡ã€‚")
        
        # é»‘è‰²/æ˜æ˜Ÿåˆ—è¡¨æŒ‰ ROAS é™åºæ’ï¼ˆæ•ˆç‡æœ€é«˜çš„æœ€å…ˆçœ‹ï¼‰
        black_list = granular_perf[granular_perf['ROAS'] > 2.0].sort_values('ROAS', ascending=False).head(20)
        st.dataframe(
            black_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                            .background_gradient(subset=['ROAS'], cmap="Greens"),
            use_container_width=True,
            hide_index=True
        )

    with tab5:
        st.subheader("æ•°æ®ä»“åº“ (Data Warehouse)")
        st.dataframe(final_df)

if __name__ == "__main__":
    main()
