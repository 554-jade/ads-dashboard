import streamlit as st
import pandas as pd
import plotly.express as px
import gspread
from google.oauth2.service_account import Credentials
from urllib.parse import urlparse
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re

# -----------------------------------------------------------------------------
# é…ç½®ä¸è¯´æ˜
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="Antigravity Ads Cloud - å¹¿å‘Šäº‘",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¤è¯é…ç½®
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

# -----------------------------------------------------------------------------
# æ•°æ®åŠ è½½ä¸å¤„ç† (ETL)
# -----------------------------------------------------------------------------

def get_gspread_client():
    """ä½¿ç”¨ Streamlit secrets è¿›è¡Œ Google Sheets è®¤è¯"""
    try:
        creds_dict = dict(st.secrets["connections"]["gsheets"])
        creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        client = gspread.authorize(creds)
        return client
    except Exception as e:
        st.error(f"è®¤è¯é”™è¯¯: {e}")
        st.stop()

@st.cache_data(ttl=600)  # ç¼“å­˜ 10 åˆ†é’Ÿ
def load_data():
    """
    ä» Google Sheets åŠ è½½æ•°æ®å¹¶æ‰§è¡ŒåŒé”®æ˜ å°„ã€‚
    """
    client = get_gspread_client()
    spreadsheet_url = st.secrets["connections"]["gsheets"]["spreadsheet"]
    
    try:
        if spreadsheet_url.startswith("http"):
            sh = client.open_by_url(spreadsheet_url)
        else:
            sh = client.open(spreadsheet_url)
            
        def read_sheet(worksheet_name, cols=None):
            ws = sh.worksheet(worksheet_name)
            data = ws.get_all_values()
            if not data:
                return pd.DataFrame()
            
            # å»é™¤è¡¨å¤´å‰åç©ºæ ¼
            headers = [h.strip() for h in data[0]]
            rows = data[1:]
            
            df = pd.DataFrame(rows, columns=headers)
            
            # å…³é”®ä¿®å¤ï¼šå»é™¤é‡å¤åˆ—åï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
            df = df.loc[:, ~df.columns.duplicated()]
            
            # å»é™¤ç©ºè¡¨å¤´åˆ—
            df = df.loc[:, df.columns != '']
            
            if cols:
                # åªæœ‰å½“åˆ—å­˜åœ¨æ—¶æ‰ç­›é€‰
                existing_cols = [c for c in cols if c in df.columns]
                if existing_cols:
                    df = df[existing_cols]
            return df
            
        raw_df = read_sheet("Raw_Data")
        manager_map = read_sheet("Manager_Map")
        category_map = read_sheet("Category_Map")

        if raw_df.empty:
            return pd.DataFrame()

        # ---------------------------------------------------------------------
        # èƒ½å¤Ÿè¯†åˆ«çš„åˆ—åæ˜ å°„ (Robust Column Mapping)
        # ---------------------------------------------------------------------
        column_mapping = {
            'è½¬åŒ–ä»·å€¼ (æŒ‰è½¬åŒ–æ—¶é—´)': 'è½¬åŒ–ä»·å€¼',
            'è½¬åŒ–ä»·å€¼ (æŒ‰è½¬åŒ–æ—¶é—´) ': 'è½¬åŒ–ä»·å€¼', # Handle trailing space
            'è½¬åŒ–ä»·å€¼ï¼ˆæŒ‰è½¬åŒ–æ—¶é—´ï¼‰': 'è½¬åŒ–ä»·å€¼', # Chinese parenthesis
            'è½¬åŒ–ä»·å€¼ï¼ˆæŒ‰è½¬åŒ–æ—¶é—´ï¼‰ ': 'è½¬åŒ–ä»·å€¼',
            'Cost': 'è´¹ç”¨',
            'Conversions': 'è½¬åŒ–æ•°',
            'Conversion value': 'è½¬åŒ–ä»·å€¼'
            # Add other known aliases here
        }
        raw_df = raw_df.rename(columns=column_mapping)
        # å…³é”®ä¿®å¤ï¼šé‡å‘½ååå¯èƒ½äº§ç”Ÿé‡å¤åˆ—ï¼ˆä¾‹å¦‚åŒæ—¶æœ‰åä¸º A å’Œ B çš„åˆ—ï¼Œéƒ½è¢«æ˜ å°„ä¸º Cï¼‰ï¼Œå†æ¬¡å»é‡
        raw_df = raw_df.loc[:, ~raw_df.columns.duplicated()]
        # ---------------------------------------------------------------------

        # åŸºç¡€æ¸…æ´—
        if 'å¤©' in raw_df.columns:
            raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'], errors='coerce')
        # åŸºç¡€æ¸…æ´—
        if 'å¤©' in raw_df.columns:
            raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'], errors='coerce')
        if 'å¹¿å‘Šè´¦å·' in raw_df.columns:
            # ç”¨æˆ·åé¦ˆ Raw Data ä¸­çš„ ID å¯èƒ½åŒ…å«åç¼€ (e.g. "ID | filename")ï¼Œä¸” split('|') å¯èƒ½å¤±æ•ˆ
            # æ”¹ç”¨æ­£åˆ™æå–æ ‡å‡†çš„ Google Ads ID æ ¼å¼ (xxx-xxx-xxxx)
            extracted_ids = raw_df['å¹¿å‘Šè´¦å·'].astype(str).str.extract(r'(\d{3}-\d{3}-\d{4})', expand=False)
            # å¦‚æœæå–åˆ°äº†å°±ç”¨æå–çš„ï¼Œæ²¡æå–åˆ°ï¼ˆå¯èƒ½æ˜¯çº¯æ•°å­—æˆ–å…¶ä»–æ ¼å¼ï¼‰å°±ä¿ç•™åŸæ ·ä½†å»é™¤ç©ºæ ¼
            raw_df['å¹¿å‘Šè´¦å·'] = extracted_ids.fillna(raw_df['å¹¿å‘Šè´¦å·'].astype(str)).str.strip()
            
            # å…³é”®ä¿®å¤ï¼šè¿‡æ»¤æ‰â€œå¹¿å‘Šè´¦å·â€ä¸ºç©ºçš„è¡Œ (ä¾‹å¦‚ Google Sheet çš„ç©ºè¡Œ)
            raw_df = raw_df[raw_df['å¹¿å‘Šè´¦å·'] != '']
            # è¿‡æ»¤æ‰éæ³•çš„ ID (ä¸åŒ…å«æ•°å­—çš„ï¼Œä¾‹å¦‚ 'value', '--')
            raw_df = raw_df[raw_df['å¹¿å‘Šè´¦å·'].astype(str).str.contains(r'\d', regex=True)]

            # ---------------------------------------------------------------------
            # è‡ªåŠ¨å»é‡ (Deduplication)
            # ---------------------------------------------------------------------
            # ç”¨æˆ·éœ€æ±‚ï¼šç›¸åŒç»´åº¦çš„æ•°æ®åº”â€œè¦†ç›–â€è€Œéç´¯åŠ ã€‚
            # ç­–ç•¥ï¼šè¯†åˆ«æ‰€æœ‰ç»´åº¦åˆ—ï¼ˆæ’é™¤å·²çŸ¥æ•°å€¼åˆ—ï¼‰ï¼Œå¯¹å®Œå…¨ç›¸åŒçš„ç»´åº¦ç»„åˆä¿ç•™æœ€åä¸€è¡Œ (keep='last')ã€‚
            
            exclude_metrics = [
                'è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'è½¬åŒ–æ•°', 'ROAS', 
                'Cost', 'Conversions', 'Conversion value', 
                'Clicks', 'Impressions', 'CTR', 'CPC', 'Views',
                'Interactions', 'Interaction rate', 'Avg. cost', 'Avg. CPM',
                'Search Impr. share', 'Display Impr. share', 'IIV', 'Invalid clicks'
            ]
            # ç­–ç•¥å‡çº§ï¼šä¸ä»…ä»…æ’é™¤å®Œå…¨åŒ¹é…çš„åˆ—ï¼Œè¿˜è¦æ’é™¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„åˆ— (Metric-like columns)
            # ä»¥é˜²æ­¢ "Avg. CPC" æˆ– "Ctr" å¤§å°å†™/ç©ºæ ¼ å·®å¼‚å¯¼è‡´å»é‡å¤±è´¥
            metric_keywords = ['cost', 'value', 'cpc', 'cpm', 'ctr', 'rate', 'clicks', 'impressions', 'conversions', 'roas', 'view']
            
            def is_metric_col(col_name):
                if col_name in exclude_metrics: return True
                c_lower = col_name.lower()
                for kw in metric_keywords:
                    if kw in c_lower:
                        return True
                return False

            dedup_subset = [c for c in raw_df.columns if not is_metric_col(c)]
            
            if dedup_subset:
                # è®°å½•å»é‡å‰è¡Œæ•°ï¼Œç”¨äº debug æˆ–æç¤º
                # before_count = len(raw_df)
                # è½¬æ¢æ‰€æœ‰ç»´åº¦åˆ—ä¸ºå­—ç¬¦ä¸²å¹¶ stripï¼Œæ¶ˆé™¤éšå½¢å·®å¼‚
                # æ³¨æ„ï¼šè¿™åªç”¨äºåˆ¤æ–­å»é‡ï¼Œä¸æ”¹å˜åŸå§‹æ•°æ®ç±»å‹ï¼Œæˆ–è€…æˆ‘ä»¬ç›´æ¥æ”¹å˜ä¹Ÿæ²¡å…³ç³»ï¼Œå› ä¸ºé€šå¸¸ç»´åº¦å°±æ˜¯å­—ç¬¦ä¸²
                # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬åªåœ¨ä¸´æ—¶ copy ä¸Šåšæ ‡å‡†åŒ– key
                
                # ä¹Ÿå¯ä»¥ç›´æ¥ inplace æ¸…æ´—ç»´åº¦åˆ—
                for col in dedup_subset:
                     raw_df[col] = raw_df[col].astype(str).str.strip()

                raw_df = raw_df.drop_duplicates(subset=dedup_subset, keep='last')
                # after_count = len(raw_df)

            # å…³é”®ä¿®å¤ï¼šå»é‡æ—¶å°†â€œå¤©â€è½¬ä¸ºäº†å­—ç¬¦ä¸²ï¼Œè¿™é‡Œå¿…é¡»è½¬å› datetimeï¼Œå¦åˆ™åç»­ç­›é€‰ä¼šæŠ¥é”™
            if 'å¤©' in raw_df.columns:
                raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'], errors='coerce')


        
        if 'å¹¿å‘Šè´¦å·' in manager_map.columns:
            manager_map['å¹¿å‘Šè´¦å·'] = manager_map['å¹¿å‘Šè´¦å·'].astype(str).str.strip() # å»é™¤ ID ç©ºæ ¼

        # 2. æ˜ å°„ä¼˜åŒ–å¸ˆ
        if 'å¹¿å‘Šè´¦å·' in raw_df.columns and 'å¹¿å‘Šè´¦å·' in manager_map.columns:
            merged_df = pd.merge(raw_df, manager_map, on='å¹¿å‘Šè´¦å·', how='left')
        else:
            merged_df = raw_df.copy()
            merged_df['ä¼˜åŒ–å¸ˆ'] = "Unknown"

        merged_df['ä¼˜åŒ–å¸ˆ'] = merged_df.get('ä¼˜åŒ–å¸ˆ', pd.Series(["Unknown"]*len(merged_df))).fillna("Unknown")
        
        # --- DEBUG: æ˜ å°„è¯Šæ–­ ---
        # å¦‚æœå¤§é‡ Unknownï¼Œç»™ç”¨æˆ·å±•ç¤ºåŸå› 
        unknown_count = (merged_df['ä¼˜åŒ–å¸ˆ'] == 'Unknown').sum()
        if unknown_count > 5:
            with st.expander(f"âš ï¸ å‘ç° {unknown_count} æ¡æ•°æ®æœªåŒ¹é…åˆ°ä¼˜åŒ–å¸ˆ (ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…)"):
                st.write("Manager Map (å‰ 5 è¡Œ):")
                st.dataframe(manager_map.head())
                st.write("Raw Data ä¸­æœªåŒ¹é…çš„å¹¿å‘Šè´¦å· (å‰ 10 ä¸ª):")
                unmatched_ids = merged_df[merged_df['ä¼˜åŒ–å¸ˆ'] == 'Unknown']['å¹¿å‘Šè´¦å·'].unique()
                st.write(unmatched_ids[:10])
                st.info("è¯·æ£€æŸ¥ 'Ad Account' (Raw_Data) å’Œ 'å¹¿å‘Šè´¦å·' (Manager_Map) æ˜¯å¦ä¸€è‡´ã€‚")
        # ---------------------

        # 3. æ˜ å°„ç±»ç›®
        def clean_url(url):
            if pd.isna(url):
                return ""
            parsed = urlparse(str(url))
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')

        if 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in merged_df.columns:
             merged_df['clean_url'] = merged_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url)
        else:
             merged_df['clean_url'] = ""

        if 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in category_map.columns:
            category_map['clean_url'] = category_map['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url)
        else:
             category_map['clean_url'] = "" # Should define clean_url column anyway
        
        if 'ç±»ç›®' in category_map.columns:
            # å…³é”®ä¿®å¤ï¼šå¦‚æœä½ ä¹‹å‰å·²ç»æŠŠ 'ç±»ç›®' å†™è¿› Raw_Data äº†ï¼Œè¿™é‡Œ merge ä¼šå¯¼è‡´ duplicate columns (ç±»ç›®_x, ç±»ç›®_y)
            # æ‰€ä»¥ merge å‰å…ˆ drop æ‰ merged_df é‡Œçš„æ—§ç±»ç›®ï¼Œä»¥ Category Map çš„æœ€æ–°æ˜ å°„ä¸ºå‡†
            if 'ç±»ç›®' in merged_df.columns:
                merged_df = merged_df.drop(columns=['ç±»ç›®'])
                
            category_map_dedup = category_map[['clean_url', 'ç±»ç›®']].drop_duplicates()
            merged_df = pd.merge(merged_df, category_map_dedup, on='clean_url', how='left')
            merged_df['ç±»ç›®'] = merged_df['ç±»ç›®'].fillna("Unknown")
        else:
            # å¦‚æœ Map é‡Œæ²¡ç±»ç›®ï¼Œä½† Raw Data é‡Œå¯èƒ½æœ‰??
            if 'ç±»ç›®' not in merged_df.columns:
                 merged_df['ç±»ç›®'] = "Unknown"
            
        if 'clean_url' in merged_df.columns:
            merged_df = merged_df.drop(columns=['clean_url'])
        
        numeric_cols = ['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS', 'è½¬åŒ–æ•°']
        # ROAS æ˜¯è®¡ç®—å‡ºæ¥çš„ï¼Œè½¬åŒ–æ•°æ˜¯å¯é€‰çš„ã€‚åªæœ‰ è´¹ç”¨å’Œè½¬åŒ–ä»·å€¼æ˜¯å¿…é¡»çš„ã€‚
        missing_numeric = [col for col in numeric_cols if col not in merged_df.columns and col not in ['è½¬åŒ–æ•°', 'ROAS']]
        
        if missing_numeric:
            st.error(f"ä¸¥é‡é”™è¯¯ï¼šæ‚¨çš„ Google Sheet 'Raw_Data' è¡¨ç¼ºå°‘ä»¥ä¸‹å…³é”®æ•°æ®åˆ—: {missing_numeric}ã€‚")
            st.write(f"å½“å‰æ£€æµ‹åˆ°çš„æ‰€æœ‰åˆ—å: {list(merged_df.columns)}")
            st.info("è¯·æ£€æŸ¥æ‚¨çš„è¡¨æ ¼è¡¨å¤´æ˜¯å¦æœ‰é”™åˆ«å­—ã€å¤šä½™ç©ºæ ¼ï¼Œæˆ–è€…åˆ—åä¸åŒ¹é…ã€‚")
            # Return empty to prevent KeyError downstream
            return pd.DataFrame()

        for col in numeric_cols:
             if col in merged_df.columns:
                merged_df[col] = merged_df[col].astype(str).str.replace(r'[^\d\.-]', '', regex=True)
                merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce').fillna(0)
        
        # å¼ºåˆ¶é‡æ–°è®¡ç®— ROASï¼Œç¡®ä¿æ•°æ®å‡†ç¡® (å³ä½¿ Sheet é‡Œæœ‰è¿™åˆ—ä½†ä¸ºç©º)
        if 'è´¹ç”¨' in merged_df.columns and 'è½¬åŒ–ä»·å€¼' in merged_df.columns:
             merged_df['ROAS'] = merged_df.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)

        return merged_df

    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return pd.DataFrame()

# -----------------------------------------------------------------------------
# æ•°æ®ä¸Šä¼ é€»è¾‘
# -----------------------------------------------------------------------------

def upload_data(uploaded_files):
    if not uploaded_files:
        return
    
    client = get_gspread_client()
    spreadsheet_url = st.secrets["connections"]["gsheets"]["spreadsheet"]
    
    if spreadsheet_url.startswith("http"):
        sh = client.open_by_url(spreadsheet_url)
    else:
        sh = client.open(spreadsheet_url)
    
    ws = sh.worksheet("Raw_Data")
    
    try:
        data = ws.get_all_values()
        if data:
            headers = [h.strip() for h in data[0]]
            existing_records = data[1:]
            if existing_records:
                current_df = pd.DataFrame(existing_records, columns=headers)
                # å…³é”®ä¿®å¤ï¼šå»é™¤é‡å¤åˆ—åï¼Œé˜²æ­¢ concat æŠ¥é”™
                current_df = current_df.loc[:, ~current_df.columns.duplicated()]
                # Filter out empty headers
                current_df = current_df.loc[:, current_df.columns != '']
            else:
                 current_df = pd.DataFrame(columns=headers)
        else:
            current_df = pd.DataFrame()
    except Exception as e:
        st.error(f"è¯»å–ç°æœ‰æ•°æ®å‡ºé”™: {e}")
        current_df = pd.DataFrame()

    # Pre-cleaning existing data for merge
    if not current_df.empty:
        # Standardize 'å¤©' column for internal processing
        if 'å¤©' in current_df.columns:
            # Keep original for now, we will normalize later
             pass
        # Ensure Ad Account is string
        if 'å¹¿å‘Šè´¦å·' in current_df.columns:
            current_df['å¹¿å‘Šè´¦å·'] = current_df['å¹¿å‘Šè´¦å·'].astype(str)

    # -------------------------------------------------------------------------
    # Process Uploaded Files
    # -------------------------------------------------------------------------
    
    # Load category map locally for processing new rows
    try:
        def clean_url_local(url):
            if pd.isna(url) or not url: return ""
            parsed = urlparse(str(url))
            return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')
            
        cat_ws = sh.worksheet("Category_Map")
        cat_data = cat_ws.get_all_values()
        if cat_data:
            cat_headers = [h.strip() for h in cat_data[0]]
            cat_df = pd.DataFrame(cat_data[1:], columns=cat_headers)
            cat_df['clean_url'] = cat_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url_local)
            cat_lookup = cat_df.set_index('clean_url')['ç±»ç›®'].to_dict()
        else:
            cat_lookup = {}
    except:
        cat_lookup = {}

    all_new_dfs = []

    for uploaded_file in uploaded_files:
        try:
            filename = uploaded_file.name
            match = re.search(r'(\d{3}-\d{3}-\d{4})', filename)
            account_id = match.group(0) if match else filename.split('.')[0].strip()
            
            df = None
            last_err = ""
            for enc in ['utf-8-sig', 'utf-16', 'utf-8', 'gbk']:
                try:
                    uploaded_file.seek(0)
                    temp_df = pd.read_csv(uploaded_file, encoding=enc, sep=None, engine='python')
                    if not temp_df.empty and len(temp_df.columns) >= 2:
                        df = temp_df
                        break
                except Exception as e:
                    last_err = str(e)
                    continue
            
            if df is None:
                st.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}ã€‚{last_err}")
                continue
            
            df['å¹¿å‘Šè´¦å·'] = account_id
            # å…¼å®¹è¡¨å¤´ä¸­å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼
            df.columns = [c.strip() for c in df.columns]
            # å…³é”®ä¿®å¤ï¼šå»é™¤ CSV ä¸­çš„é‡å¤åˆ—å
            df = df.loc[:, ~df.columns.duplicated()]

            # Date Normalization
            if 'å¤©' in df.columns:
                df['å¤©'] = pd.to_datetime(df['å¤©'], errors='coerce').dt.strftime('%Y-%m-%d')
                df = df.dropna(subset=['å¤©'])
            
            # Enrich Data
            # 1. Category
            if 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in df.columns:
                df['ç±»ç›®'] = df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(lambda x: cat_lookup.get(clean_url_local(x), "Unknown"))
            else:
                 df['ç±»ç›®'] = "Unknown"

            # 2. Ad Group ID
            # Ensure columns exist with default empty string
            for col in ['å¹¿å‘Šç³»åˆ—', 'å¹¿å‘Šç»„', 'æœ€ç»ˆåˆ°è¾¾ç½‘å€']:
                if col not in df.columns:
                    df[col] = ""
            
            df['å¹¿å‘Šç»„id'] = df['å¹¿å‘Šè´¦å·'].astype(str) + df['å¹¿å‘Šç³»åˆ—'].astype(str) + df['ç±»ç›®'].astype(str) + df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].astype(str) + df['å¹¿å‘Šç»„'].astype(str)
            
            all_new_dfs.append(df)
            
        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {e}")

    # -------------------------------------------------------------------------
    # Merge, Dedup, and Overwrite
    # -------------------------------------------------------------------------
    if all_new_dfs:
        new_combined_df = pd.concat(all_new_dfs, ignore_index=True)
        
        # Combine Old and New
        if not current_df.empty:
            # Align schema - add missing cols to current_df if new data has them (and vice versa)
            full_df = pd.concat([current_df, new_combined_df], ignore_index=True)
        else:
            full_df = new_combined_df

        # DEFINITIVE DEDUPLICATION
        # 1. Identify Metrics (to exclude from key)
        exclude_metrics = [
            'è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'è½¬åŒ–æ•°', 'ROAS', 
            'Cost', 'Conversions', 'Conversion value', 
            'Clicks', 'Impressions', 'CTR', 'CPC', 'Views',
            'Interactions', 'Interaction rate', 'Avg. cost', 'Avg. CPM',
            'Search Impr. share', 'Display Impr. share', 'IIV', 'Invalid clicks'
        ]
        metric_keywords = ['cost', 'value', 'cpc', 'cpm', 'ctr', 'rate', 'clicks', 'impressions', 'conversions', 'roas', 'view']
        
        def is_metric_col(col_name):
            if col_name in exclude_metrics: return True
            c_lower = col_name.lower()
            return any(kw in c_lower for kw in metric_keywords)

        # 2. Clean Dimensions for Key Generation
        dedup_subset = [c for c in full_df.columns if not is_metric_col(c)]
        
        if dedup_subset:
            # Create a temporary Normalized Key for dedup
            msg_cols = [c for c in dedup_subset if c in full_df.columns]
            
            # Helper to normalize for dedup ONLY (without changing actual data)
            # Actually, to be safe, let's normalize the ID column in the data itself
            if 'å¹¿å‘Šè´¦å·' in full_df.columns:
                 # Extract standard ID format
                 full_df['å¹¿å‘Šè´¦å·'] = full_df['å¹¿å‘Šè´¦å·'].astype(str).str.extract(r'(\d{3}-\d{3}-\d{4})', expand=False).fillna(full_df['å¹¿å‘Šè´¦å·']).str.strip()
            
            for c in msg_cols:
                full_df[c] = full_df[c].astype(str).str.strip()

            before_len = len(full_df)
            full_df = full_df.drop_duplicates(subset=msg_cols, keep='last')
            after_len = len(full_df)
            st.info(f"æ•°æ®åˆå¹¶ç»Ÿè®¡: åˆå¹¶å‰ {before_len} è¡Œ -> è¦†ç›–å»é‡å {after_len} è¡Œ (å‡å°‘ {before_len - after_len} è¡Œ)")

        # 3. Write Back to Sheet (CLEAR + UPDATE)
        # Handle NaN before writing
        full_df = full_df.fillna("")
        
        try:
            ws.clear()
            
            # 1. Update headers
            updated_headers = full_df.columns.tolist()
            ws.update(range_name='A1', values=[updated_headers])
            
            # 2. Values - Batch Upload to avoid Proxy Timeout
            updated_values = full_df.astype(str).values.tolist()
            total_rows = len(updated_values)
            chunk_size = 1000 # æ¯æ¬¡ä¸Šä¼  1000 è¡Œ
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i in range(0, total_rows, chunk_size):
                chunk = updated_values[i : i + chunk_size]
                # append_rows è‡ªåŠ¨å¤„ç†è¡Œå·ï¼Œæ¯”è®¡ç®— range æ›´ç¨³å¥
                ws.append_rows(chunk)
                
                # Update progress
                progress = min((i + chunk_size) / total_rows, 1.0)
                progress_bar.progress(progress)
                status_text.text(f"æ­£åœ¨ä¸Šä¼ æ•°æ®: {min(i + chunk_size, total_rows)} / {total_rows} è¡Œ...")
                
            status_text.empty()
            progress_bar.empty()
            
            st.success(f"æ•°æ®æ›´æ–°æˆåŠŸï¼å·²è¦†ç›–å†™å…¥ {total_rows} è¡Œæ•°æ®ã€‚")
            st.cache_data.clear()
            
        except Exception as e:
            st.error(f"å†™å…¥ Google Sheet å¤±è´¥: {e}")
            
    else:
        st.warning("æ²¡æœ‰è¯»å–åˆ°æœ‰æ•ˆçš„æ–°æ•°æ®ã€‚")

# -----------------------------------------------------------------------------
# ä¸»åº”ç”¨ç¨‹åº
# -----------------------------------------------------------------------------

def main():
    st.title("Antigravity Ads Cloud ğŸš€ - å¹¿å‘Šäº‘")
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    if df.empty:
        st.warning("æœªæ‰¾åˆ°æ•°æ®æˆ–è¿æ¥å¤±è´¥ã€‚å¯èƒ½æ˜¯ç½‘ç»œæ³¢åŠ¨ï¼Œè¯·å°è¯•åˆ·æ–°ã€‚")
        if st.button("ğŸ”„ é‡è¯•è¿æ¥ (Retry)"):
            st.cache_data.clear()
            st.rerun()
        with st.sidebar:
            st.header("æ•°æ®ä¸Šä¼ ")
            uploaded_files = st.file_uploader("ä¸Šä¼  CSV", accept_multiple_files=True, type="csv")
            if uploaded_files:
                if st.button("å¤„ç†å¹¶ä¸Šä¼ "):
                    upload_data(uploaded_files)
        return

    # -------------------------------------------------------------------------
    # ä¾§è¾¹æ ç­›é€‰å™¨
    # -------------------------------------------------------------------------
    st.sidebar.header("å…¨å±€ç­›é€‰å™¨")
    
    if st.sidebar.button("ğŸ”„ åˆ·æ–°æ•°æ® (é‡ç½®ç¼“å­˜)"):
        st.cache_data.clear()
        st.rerun()
    
    min_date = df['å¤©'].min().date()
    max_date = df['å¤©'].max().date()
    start_date = st.sidebar.date_input("å¼€å§‹æ—¥æœŸ", value=max(min_date, max_date - pd.Timedelta(days=30)))
    end_date = st.sidebar.date_input("ç»“æŸæ—¥æœŸ", value=max_date)

    mask_date = (df['å¤©'].dt.date >= start_date) & (df['å¤©'].dt.date <= end_date)
    df_filtered_date = df[mask_date]

    managers = ["æ•´ä½“"] + sorted(df_filtered_date['ä¼˜åŒ–å¸ˆ'].unique().tolist())
    selected_managers = st.sidebar.multiselect("ä¼˜åŒ–å¸ˆ", managers, default=["æ•´ä½“"])
    
    if "æ•´ä½“" in selected_managers:
        df_filtered_manager = df_filtered_date
    else:
        df_filtered_manager = df_filtered_date[df_filtered_date['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)]
    
    categories = sorted(df_filtered_manager['ç±»ç›®'].unique().tolist())
    selected_categories = st.sidebar.multiselect("ç±»ç›®", categories, default=categories)
    
    df_filtered_category = df_filtered_manager[df_filtered_manager['ç±»ç›®'].isin(selected_categories)]
    
    accounts = sorted(df_filtered_category['å¹¿å‘Šè´¦å·'].unique().tolist())
    selected_accounts = st.sidebar.multiselect("å¹¿å‘Šè´¦å·", accounts, default=accounts)
    
    final_df = df_filtered_category[df_filtered_category['å¹¿å‘Šè´¦å·'].isin(selected_accounts)]

    st.sidebar.markdown("---")
    st.sidebar.header("æ•°æ®ä¸Šä¼ ")
    uploaded_files = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", accept_multiple_files=True, type="csv")
    if uploaded_files:
        if st.button("å¤„ç†å¹¶ä¸Šä¼ "):
            upload_data(uploaded_files)

    # -------------------------------------------------------------------------
    # Tabs
    # -------------------------------------------------------------------------
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "æŒ‡æŒ¥ä¸­å¿ƒ", "å›¢é˜Ÿä¸æˆ˜ç•¥", "æ·±åº¦é€è§†", "çº¢é»‘æ¦œ (å¼‚å¸¸è¯Šæ–­)", "æ•°æ®ä»“åº“"
    ])

    with tab1:
        st.subheader("æŒ‡æŒ¥ä¸­å¿ƒ (Command Center)")
        
        current_spend = final_df['è´¹ç”¨'].sum()
        current_val = final_df['è½¬åŒ–ä»·å€¼'].sum()
        current_roas = current_val / current_spend if current_spend > 0 else 0
        
        has_conversions = 'è½¬åŒ–æ•°' in final_df.columns
        current_conversions = final_df['è½¬åŒ–æ•°'].sum() if has_conversions else 1
        current_cpa = current_spend / current_conversions if has_conversions and current_conversions > 0 else 0

        days_diff = (end_date - start_date).days + 1
        prev_start = start_date - pd.Timedelta(days=days_diff)
        prev_end = start_date - pd.Timedelta(days=1)
        
        mask_prev = (df['å¤©'].dt.date >= prev_start) & (df['å¤©'].dt.date <= prev_end) & \
                    (df['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)) & \
                    (df['ç±»ç›®'].isin(selected_categories)) & \
                    (df['å¹¿å‘Šè´¦å·'].isin(selected_accounts))
        prev_df = df[mask_prev]
        
        prev_spend = prev_df['è´¹ç”¨'].sum()
        prev_val = prev_df['è½¬åŒ–ä»·å€¼'].sum()
        prev_roas = prev_val / prev_spend if prev_spend > 0 else 0
        
        permil_delta_spend = ((current_spend - prev_spend) / prev_spend) * 100 if prev_spend > 0 else 0
        permil_delta_roas = ((current_roas - prev_roas) / prev_roas) * 100 if prev_roas > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        col1.metric("æ€»æ¶ˆè€— (Total Spend)", f"${current_spend:,.2f}", f"{permil_delta_spend:.2f}%")
        col2.metric("æ•´ä½“ ROAS", f"{current_roas:.2f}", f"{permil_delta_roas:.2f}%")
        col3.metric("æ€»è½¬åŒ–ä»·å€¼ (Total Value)", f"${current_val:,.2f}") 

        st.markdown("### ä¸šç»©è¶‹åŠ¿ (Performance Trend)")
        daily_trend = final_df.groupby('å¤©').agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        daily_trend['ROAS'] = daily_trend['è½¬åŒ–ä»·å€¼'] / daily_trend['è´¹ç”¨']
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(
            go.Bar(x=daily_trend['å¤©'], y=daily_trend['è´¹ç”¨'], name="æ¶ˆè€— (Spend)"),
            secondary_y=False,
        )
        fig.add_trace(
            go.Scatter(x=daily_trend['å¤©'], y=daily_trend['ROAS'], name="ROAS", mode='lines+markers'),
            secondary_y=True,
        )
        fig.update_layout(title_text="æ¶ˆè€— vs ROAS è¶‹åŠ¿")
        fig.update_yaxes(title_text="æ¶ˆè€— (Spend)", secondary_y=False)
        fig.update_yaxes(title_text="ROAS", secondary_y=True)
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        st.subheader("å›¢é˜Ÿä¸æˆ˜ç•¥ (Team & Strategy)")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### äººæ•ˆçŸ©é˜µ (People Matrix)")
            manager_perf = final_df.groupby('ä¼˜åŒ–å¸ˆ').agg({
                'è´¹ç”¨': 'sum', 
                'è½¬åŒ–ä»·å€¼': 'sum'
            }).reset_index()
            manager_perf['ROAS'] = manager_perf['è½¬åŒ–ä»·å€¼'] / manager_perf['è´¹ç”¨']
            
            fig_bubble = px.scatter(
                manager_perf, x="è´¹ç”¨", y="ROAS", size="è½¬åŒ–ä»·å€¼", color="ä¼˜åŒ–å¸ˆ",
                hover_name="ä¼˜åŒ–å¸ˆ", title="ä¼˜åŒ–å¸ˆè¡¨ç°çŸ©é˜µ", size_max=60
            )
            fig_bubble.add_hline(y=1.0, line_dash="dash", line_color="red")
            st.plotly_chart(fig_bubble, use_container_width=True)
            
        with col2:
            st.markdown("#### å“ç±»ç‰ˆå›¾ (Category Share)")
            cat_perf = final_df.groupby('ç±»ç›®').agg({'è´¹ç”¨': 'sum'}).reset_index()
            fig_pie = px.pie(cat_perf, values='è´¹ç”¨', names='ç±»ç›®', title="å„å“ç±»æ¶ˆè€—å æ¯”")
            st.plotly_chart(fig_pie, use_container_width=True)

    with tab3:
        st.subheader("æ·±åº¦é€è§† (Deep Pivot)")
        c1, c2 = st.columns(2)
        with c1:
            pivot_rows = st.multiselect("è¡Œç»´åº¦", ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¤©', 'å¹¿å‘Šè´¦å·'], default=['ä¼˜åŒ–å¸ˆ'])
        with c2:
            pivot_vals = st.multiselect("æ•°å€¼æŒ‡æ ‡", ['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'], default=['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'])
            
        if pivot_rows and pivot_vals:
            pivot_df = final_df.groupby(pivot_rows)[['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼']].sum().reset_index()
            pivot_df['ROAS'] = pivot_df['è½¬åŒ–ä»·å€¼'] / pivot_df['è´¹ç”¨']
            display_cols = pivot_rows + pivot_vals
            styler = pivot_df[display_cols].style
            if 'ROAS' in display_cols:
                styler = styler.background_gradient(subset=['ROAS'], cmap="RdYlGn", vmin=0.5, vmax=2.0)
            
            st.dataframe(
                styler,
                use_container_width=True
            )

    with tab4:
        st.subheader("çº¢é»‘æ¦œ (Red/Black List)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»†åˆ†ç»´åº¦çš„åˆ—
        granular_cols = ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·'] 
        if 'å¹¿å‘Šç»„id' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„id')
        if 'å¹¿å‘Šç³»åˆ—' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç³»åˆ—')
        if 'å¹¿å‘Šç»„' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„')
            
        # èšåˆæ•°æ®
        granular_perf = final_df.groupby(granular_cols).agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        granular_perf['ROAS'] = granular_perf.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)
        
        # æ•´ç†åˆ—é¡ºåº
        display_order = [c for c in ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·', 'å¹¿å‘Šç³»åˆ—', 'å¹¿å‘Šç»„', 'å¹¿å‘Šç»„id', 'è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'] if c in granular_perf.columns]
        granular_perf = granular_perf[display_order]

        # æ”¹ä¸ºå‚ç›´æ’åˆ— (Vertical Layout)
        st.markdown("### ğŸ”´ äºæŸæ¦œ (æŒ‰æ¶ˆè€—é™åº - æ­¢æŸä¼˜å…ˆçº§)")
        st.markdown("âš ï¸ **æ­¢æŸå»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹æ¶ˆè€—é«˜ä¸” ROAS < 1.7ï¼Œåº”ä¼˜å…ˆæ£€æŸ¥ç´ ææˆ–å…³åœã€‚")
        
        # çº¢è‰²åˆ—è¡¨æŒ‰è´¹ç”¨é™åºæ’ï¼ˆäºæŸæœ€å¤šçš„æœ€å…ˆçœ‹ï¼‰
        red_list = granular_perf[(granular_perf['ROAS'] < 1.7) & (granular_perf['è´¹ç”¨'] > 0)].sort_values('è´¹ç”¨', ascending=False).head(20)
        st.dataframe(
            red_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                          .background_gradient(subset=['è´¹ç”¨'], cmap="Reds"),
            use_container_width=True,
            hide_index=True
        )
        
        st.markdown("---") # Divider
        
        st.markdown("### ğŸŒŸ æ˜æ˜Ÿæ¦œ (æŒ‰ ROAS é™åº - æ‰©é‡ä¼˜å…ˆçº§)")
        st.markdown("ğŸ’¡ **æ‰©é‡å»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹ ROAS > 2.0ï¼Œæ•ˆç‡æé«˜ï¼Œå¯åœ¨ä¿æŒç¨³å®šçš„å‰æä¸‹é€‚åº¦æ‰©é‡ã€‚")
        
        # é»‘è‰²/æ˜æ˜Ÿåˆ—è¡¨æŒ‰ ROAS é™åºæ’ï¼ˆæ•ˆç‡æœ€é«˜çš„æœ€å…ˆçœ‹ï¼‰
        black_list = granular_perf[granular_perf['ROAS'] > 2.0].sort_values('ROAS', ascending=False).head(20)
        st.dataframe(
            black_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                            .background_gradient(subset=['ROAS'], cmap="Greens"),
            use_container_width=True,
            hide_index=True
        )

    with tab5:
        st.subheader("æ•°æ®ä»“åº“ (Data Warehouse)")
        st.dataframe(final_df)

if __name__ == "__main__":
    main()
