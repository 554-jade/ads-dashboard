import streamlit as st
import pandas as pd
import plotly.express as px
import gspread
from google.oauth2.service_account import Credentials
from urllib.parse import urlparse
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re

# -----------------------------------------------------------------------------
# é…ç½®ä¸è¯´æ˜
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="Antigravity Ads Cloud - å¹¿å‘Šäº‘",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¤è¯é…ç½®
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]


from sqlalchemy import create_engine, text

# -----------------------------------------------------------------------------
# æ•°æ®åŠ è½½ä¸å¤„ç† (ETL)
# -----------------------------------------------------------------------------

def get_gspread_client():
    """ä½¿ç”¨ Streamlit secrets è¿›è¡Œ Google Sheets è®¤è¯"""
    try:
        creds_dict = dict(st.secrets["connections"]["gsheets"])
        creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        client = gspread.authorize(creds)
        return client
    except Exception as e:
        st.error(f"è®¤è¯é”™è¯¯: {e}")
        st.stop()

def get_db_connection():
    """Create MySQL connection using SQLAlchemy"""
    try:
        db_config = st.secrets["connections"]["mysql"]
        # Format: mysql+pymysql://user:password@host:port/database
        connection_str = f"mysql+pymysql://{db_config['username']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}"
        engine = create_engine(connection_str)
        return engine.connect()
    except Exception as e:
        st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        st.stop()




@st.cache_data(ttl=600)
def load_data():
    """
    ä» MySQL åŠ è½½ Campaign çº§åˆ«æ•°æ® (t_google_cost)ï¼Œå¹¶è¯»å–æœ¬åœ° Excel æ˜ å°„è¡¨ã€‚
    """
    # 1. åŠ è½½æ˜ å°„è¡¨ (From Local Excel)
    manager_map = pd.DataFrame()
    
    excel_path = "Ads_BI/mapping.xlsx"
    try:
        # Load Manager Map
        # å‡è®¾ Excel ä¸­æœ‰åä¸º "Manager_Map" çš„ sheetï¼Œæˆ–è€…æˆ‘ä»¬è¯»å–ç¬¬ä¸€ä¸ªåŒ…å« "å¹¿å‘Šè´¦å·" çš„ sheet
        xls = pd.ExcelFile(excel_path)
        sheet_names = xls.sheet_names
        
        target_sheet = None
        for s in sheet_names:
            if "manager" in s.lower() or "ä¼˜åŒ–å¸ˆ" in s:
                target_sheet = s
                break
        if not target_sheet and sheet_names:
            target_sheet = sheet_names[0] # Fallback
            
        if target_sheet:
            manager_map = pd.read_excel(xls, sheet_name=target_sheet)
            # Normalize Columns
            manager_map.columns = [c.strip() for c in manager_map.columns]
            # Ensure required columns exist
            if 'å¹¿å‘Šè´¦å·' in manager_map.columns and 'ä¼˜åŒ–å¸ˆ' in manager_map.columns:
                 # Standardize ID: remove all non-digits for robust matching
                 manager_map['join_id'] = manager_map['å¹¿å‘Šè´¦å·'].astype(str).str.replace(r'\D', '', regex=True)
            else:
                 st.warning(f"æ˜ å°„è¡¨ {target_sheet} ç¼ºå°‘ 'å¹¿å‘Šè´¦å·' æˆ– 'ä¼˜åŒ–å¸ˆ' åˆ—")
                 manager_map = pd.DataFrame()
        
    except Exception as e:
        st.error(f"åŠ è½½æœ¬åœ°æ˜ å°„è¡¨å¤±è´¥: {e}")
    

    # 1.2 åŠ è½½ Campaign -> URL æ˜ å°„è¡¨ (Bridge Map)
    bridge_map = pd.DataFrame()
    try:
        # Look for sheet "å¹¿å‘Šmapping"
        if "å¹¿å‘Šmapping" in sheet_names:
            bridge_df = pd.read_excel(xls, sheet_name="å¹¿å‘Šmapping")
            bridge_df.columns = [c.strip() for c in bridge_df.columns]
            if 'å¹¿å‘Šç³»åˆ—' in bridge_df.columns and 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in bridge_df.columns:
                # Create dictionary: Campaign -> URL
                # Handle duplicates: take first or last? Let's take first non-empty.
                bridge_df = bridge_df.dropna(subset=['å¹¿å‘Šç³»åˆ—'])
                # Clean keys
                bridge_df['å¹¿å‘Šç³»åˆ—'] = bridge_df['å¹¿å‘Šç³»åˆ—'].astype(str).str.strip()
                bridge_map = bridge_df.set_index('å¹¿å‘Šç³»åˆ—')['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].to_dict()
    except Exception as e:
         st.warning(f"åŠ è½½å¹¿å‘Šæ˜ å°„è¡¨å¤±è´¥: {e}")

    # 1.3 åŠ è½½ URL -> Category æ˜ å°„è¡¨ (Category Map)
    category_map_dict = {}
    try:
        # Look for sheet "Category_Map"
        # Check if it exists in xls (Local) OR fetch from GSheets? 
        # User said "updated mapping this table", implying it's in the same excel file.
        cat_sheet = next((s for s in sheet_names if "category" in s.lower() or "ç±»ç›®" in s), None)
        
        if cat_sheet:
            cat_df = pd.read_excel(xls, sheet_name=cat_sheet)
            cat_df.columns = [c.strip() for c in cat_df.columns]
            
            def clean_url_local(url):
                if pd.isna(url) or not url: return ""
                try:
                    parsed = urlparse(str(url))
                    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')
                except:
                    return ""

            if 'æœ€ç»ˆåˆ°è¾¾ç½‘å€' in cat_df.columns and 'ç±»ç›®' in cat_df.columns:
                cat_df['clean_url'] = cat_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(clean_url_local)
                category_map_dict = cat_df.set_index('clean_url')['ç±»ç›®'].to_dict()
    except Exception as e:
         st.warning(f"åŠ è½½ç±»ç›®æ˜ å°„è¡¨å¤±è´¥: {e}")
         
    
    # 2. åŠ è½½å¹¿å‘Šæ•°æ® (From MySQL - Campaign Level)
    try:
        conn = get_db_connection()
        # t_google_cost schema: day_time, customer_id, campaign_name, cost, conversions, all_conversion_value
        query = """
            SELECT 
                day_time as 'å¤©',
                customer_id as 'å¹¿å‘Šè´¦å·',
                campaign_name as 'å¹¿å‘Šç³»åˆ—',
                cost as 'è´¹ç”¨',
                conversions as 'è½¬åŒ–æ•°',
                conversions_value_by_conversion_date as 'è½¬åŒ–ä»·å€¼'
            FROM t_google_cost
            WHERE day_time >= DATE_SUB(CURDATE(), INTERVAL 90 DAY)
        """
        raw_df = pd.read_sql(text(query), conn)
        conn.close()
        
        if raw_df.empty:
            return pd.DataFrame()

        # æ•°æ®ç±»å‹è½¬æ¢ä¸æ¸…æ´—
        raw_df['å¤©'] = pd.to_datetime(raw_df['å¤©'])
        
        numeric_cols = ['è´¹ç”¨', 'è½¬åŒ–æ•°', 'è½¬åŒ–ä»·å€¼']
        for col in numeric_cols:
            raw_df[col] = pd.to_numeric(raw_df[col], errors='coerce').fillna(0)
            
        # å¼ºåˆ¶è®¡ç®— ROAS
        raw_df['ROAS'] = raw_df.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)
        
        # Standardize ID for joining
        raw_df['join_id'] = raw_df['å¹¿å‘Šè´¦å·'].astype(str).str.replace(r'\D', '', regex=True)

    except Exception as e:
        st.error(f"æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
        return pd.DataFrame()

    # 3. æ•°æ®åˆå¹¶
    
    # 3.1 ä¼˜åŒ–å¸ˆæ˜ å°„ (Manager)
    if not manager_map.empty:
        map_dedup = manager_map[['join_id', 'ä¼˜åŒ–å¸ˆ']].drop_duplicates(subset=['join_id'])
        merged_df = pd.merge(raw_df, map_dedup, on='join_id', how='left')
        merged_df['ä¼˜åŒ–å¸ˆ'] = merged_df['ä¼˜åŒ–å¸ˆ'].fillna("Unknown")
    else:
        merged_df = raw_df.copy()
        merged_df['ä¼˜åŒ–å¸ˆ'] = "Unknown"

    
    # 3.2 ç±»ç›®æ˜ å°„ (Category) - VIA BRIDGE
    # Step A: Map Campaign -> URL
    def get_url_from_campaign(row):
        camp_name = str(row.get('å¹¿å‘Šç³»åˆ—', '')).strip()
        return bridge_map.get(camp_name, "")
    
    merged_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'] = merged_df.apply(get_url_from_campaign, axis=1)
    
    # Step B: Map URL -> Category
    def get_category_from_url(url):
        if not url: return "Unknown"
        # Clean URL
        try:
            parsed = urlparse(str(url))
            clean = f"{parsed.scheme}://{parsed.netloc}{parsed.path}".rstrip('/')
            return category_map_dict.get(clean, "Unknown")
        except:
             return "Unknown"

    merged_df['ç±»ç›®'] = merged_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'].apply(get_category_from_url)
    
    # Fallback to inference if "Unknown"
    def infer_category_fallback(row):
        if row['ç±»ç›®'] != "Unknown":
            return row['ç±»ç›®']
        
        # Fallback to name inference
        name = str(row.get('å¹¿å‘Šç³»åˆ—', '')).lower()
        if 'shopping' in name: return 'Shopping'
        elif 'search' in name: return 'Search'
        elif 'pmax' in name: return 'PMax'
        elif 'brand' in name: return 'Brand'
        elif 'display' in name: return 'Display'
        elif 'youtube' in name or 'video' in name: return 'Video'
        return 'Other'

    merged_df['ç±»ç›®'] = merged_df.apply(infer_category_fallback, axis=1)


    


    # 3.3 è¡¥å…¨ç¼ºå¤±åˆ—ä»¥å…¼å®¹åç»­é€»è¾‘
    merged_df['å¹¿å‘Šç»„'] = "All"
    merged_df['æœ€ç»ˆåˆ°è¾¾ç½‘å€'] = ""
    merged_df['clean_url'] = ""

    # 3.4 ç”Ÿæˆ ID
    merged_df['å¹¿å‘Šç»„id'] = (
        merged_df['å¹¿å‘Šè´¦å·'].astype(str) + "_" +
        merged_df['ç±»ç›®'].astype(str) + "_" +  # Added Category to ID for uniqueness
        merged_df['å¹¿å‘Šç³»åˆ—'].astype(str)
    )

    return merged_df




# ä¸»åº”ç”¨ç¨‹åº
# -----------------------------------------------------------------------------

def main():
    st.title("Antigravity Ads Cloud ğŸš€ - å¹¿å‘Šäº‘")
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    if df.empty:
        st.warning("æœªæ‰¾åˆ°æ•°æ®æˆ–è¿æ¥å¤±è´¥ã€‚å¯èƒ½æ˜¯ç½‘ç»œæ³¢åŠ¨ï¼Œè¯·å°è¯•åˆ·æ–°ã€‚")
        if st.button("ğŸ”„ é‡è¯•è¿æ¥ (Retry)"):
            st.cache_data.clear()
            st.rerun()
        return

    # -------------------------------------------------------------------------
    # ä¾§è¾¹æ ç­›é€‰å™¨
    # -------------------------------------------------------------------------
    st.sidebar.header("å…¨å±€ç­›é€‰å™¨")
    
    if st.sidebar.button("ğŸ”„ åˆ·æ–°æ•°æ® (é‡ç½®ç¼“å­˜)"):
        st.cache_data.clear()
        st.rerun()
    
    min_date = df['å¤©'].min().date()
    max_date = df['å¤©'].max().date()
    start_date = st.sidebar.date_input("å¼€å§‹æ—¥æœŸ", value=max(min_date, max_date - pd.Timedelta(days=30)))
    end_date = st.sidebar.date_input("ç»“æŸæ—¥æœŸ", value=max_date)

    mask_date = (df['å¤©'].dt.date >= start_date) & (df['å¤©'].dt.date <= end_date)
    df_filtered_date = df[mask_date]

    managers = ["æ•´ä½“"] + sorted(df_filtered_date['ä¼˜åŒ–å¸ˆ'].unique().tolist())
    selected_managers = st.sidebar.multiselect("ä¼˜åŒ–å¸ˆ", managers, default=["æ•´ä½“"])
    
    if "æ•´ä½“" in selected_managers:
        df_filtered_manager = df_filtered_date
    else:
        df_filtered_manager = df_filtered_date[df_filtered_date['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)]
    
    categories = sorted(df_filtered_manager['ç±»ç›®'].unique().tolist())
    selected_categories = st.sidebar.multiselect("ç±»ç›®", categories, default=categories)
    
    df_filtered_category = df_filtered_manager[df_filtered_manager['ç±»ç›®'].isin(selected_categories)]
    
    accounts = sorted(df_filtered_category['å¹¿å‘Šè´¦å·'].unique().tolist())
    selected_accounts = st.sidebar.multiselect("å¹¿å‘Šè´¦å·", accounts, default=accounts)
    
    final_df = df_filtered_category[df_filtered_category['å¹¿å‘Šè´¦å·'].isin(selected_accounts)]



    # -------------------------------------------------------------------------
    # Tabs
    # -------------------------------------------------------------------------
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "æŒ‡æŒ¥ä¸­å¿ƒ", "å›¢é˜Ÿä¸æˆ˜ç•¥", "æ·±åº¦é€è§†", "çº¢é»‘æ¦œ (å¼‚å¸¸è¯Šæ–­)", "æ•°æ®ä»“åº“"
    ])

    with tab1:
        st.subheader("æŒ‡æŒ¥ä¸­å¿ƒ (Command Center)")
        
        current_spend = final_df['è´¹ç”¨'].sum()
        current_val = final_df['è½¬åŒ–ä»·å€¼'].sum()
        current_roas = current_val / current_spend if current_spend > 0 else 0
        
        has_conversions = 'è½¬åŒ–æ•°' in final_df.columns
        current_conversions = final_df['è½¬åŒ–æ•°'].sum() if has_conversions else 1
        current_cpa = current_spend / current_conversions if has_conversions and current_conversions > 0 else 0

        days_diff = (end_date - start_date).days + 1
        prev_start = start_date - pd.Timedelta(days=days_diff)
        prev_end = start_date - pd.Timedelta(days=1)
        
        mask_prev = (df['å¤©'].dt.date >= prev_start) & (df['å¤©'].dt.date <= prev_end) & \
                    (df['ä¼˜åŒ–å¸ˆ'].isin(selected_managers)) & \
                    (df['ç±»ç›®'].isin(selected_categories)) & \
                    (df['å¹¿å‘Šè´¦å·'].isin(selected_accounts))
        prev_df = df[mask_prev]
        
        prev_spend = prev_df['è´¹ç”¨'].sum()
        prev_val = prev_df['è½¬åŒ–ä»·å€¼'].sum()
        prev_roas = prev_val / prev_spend if prev_spend > 0 else 0
        
        permil_delta_spend = ((current_spend - prev_spend) / prev_spend) * 100 if prev_spend > 0 else 0
        permil_delta_roas = ((current_roas - prev_roas) / prev_roas) * 100 if prev_roas > 0 else 0
        
        col1, col2, col3 = st.columns(3)
        col1.metric("æ€»æ¶ˆè€— (Total Spend)", f"${current_spend:,.2f}", f"{permil_delta_spend:.2f}%")
        col2.metric("æ•´ä½“ ROAS", f"{current_roas:.2f}", f"{permil_delta_roas:.2f}%")
        col3.metric("æ€»è½¬åŒ–ä»·å€¼ (Total Value)", f"${current_val:,.2f}") 

        st.markdown("### ä¸šç»©è¶‹åŠ¿ (Performance Trend)")
        daily_trend = final_df.groupby('å¤©').agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        daily_trend['ROAS'] = daily_trend['è½¬åŒ–ä»·å€¼'] / daily_trend['è´¹ç”¨']
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(
            go.Bar(x=daily_trend['å¤©'], y=daily_trend['è´¹ç”¨'], name="æ¶ˆè€— (Spend)"),
            secondary_y=False,
        )
        fig.add_trace(
            go.Scatter(x=daily_trend['å¤©'], y=daily_trend['ROAS'], name="ROAS", mode='lines+markers'),
            secondary_y=True,
        )
        fig.update_layout(title_text="æ¶ˆè€— vs ROAS è¶‹åŠ¿")
        fig.update_yaxes(title_text="æ¶ˆè€— (Spend)", secondary_y=False)
        fig.update_yaxes(title_text="ROAS", secondary_y=True)
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        st.subheader("å›¢é˜Ÿä¸æˆ˜ç•¥ (Team & Strategy)")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### äººæ•ˆçŸ©é˜µ (People Matrix)")
            manager_perf = final_df.groupby('ä¼˜åŒ–å¸ˆ').agg({
                'è´¹ç”¨': 'sum', 
                'è½¬åŒ–ä»·å€¼': 'sum'
            }).reset_index()
            manager_perf['ROAS'] = manager_perf['è½¬åŒ–ä»·å€¼'] / manager_perf['è´¹ç”¨']
            
            fig_bubble = px.scatter(
                manager_perf, x="è´¹ç”¨", y="ROAS", size="è½¬åŒ–ä»·å€¼", color="ä¼˜åŒ–å¸ˆ",
                hover_name="ä¼˜åŒ–å¸ˆ", title="ä¼˜åŒ–å¸ˆè¡¨ç°çŸ©é˜µ", size_max=60
            )
            fig_bubble.add_hline(y=1.0, line_dash="dash", line_color="red")
            st.plotly_chart(fig_bubble, use_container_width=True)
            
        with col2:
            st.markdown("#### å“ç±»ç‰ˆå›¾ (Category Share)")
            cat_perf = final_df.groupby('ç±»ç›®').agg({'è´¹ç”¨': 'sum'}).reset_index()
            fig_pie = px.pie(cat_perf, values='è´¹ç”¨', names='ç±»ç›®', title="å„å“ç±»æ¶ˆè€—å æ¯”")
            st.plotly_chart(fig_pie, use_container_width=True)

    with tab3:
        st.subheader("æ·±åº¦é€è§† (Deep Pivot)")
        c1, c2 = st.columns(2)
        with c1:
            pivot_rows = st.multiselect("è¡Œç»´åº¦", ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¤©', 'å¹¿å‘Šè´¦å·'], default=['ä¼˜åŒ–å¸ˆ'])
        with c2:
            pivot_vals = st.multiselect("æ•°å€¼æŒ‡æ ‡", ['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'], default=['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'])
            
        if pivot_rows and pivot_vals:
            pivot_df = final_df.groupby(pivot_rows)[['è´¹ç”¨', 'è½¬åŒ–ä»·å€¼']].sum().reset_index()
            pivot_df['ROAS'] = pivot_df['è½¬åŒ–ä»·å€¼'] / pivot_df['è´¹ç”¨']
            display_cols = pivot_rows + pivot_vals
            styler = pivot_df[display_cols].style
            if 'ROAS' in display_cols:
                styler = styler.background_gradient(subset=['ROAS'], cmap="RdYlGn", vmin=0.5, vmax=2.0)
            
            st.dataframe(
                styler,
                use_container_width=True
            )

    with tab4:
        st.subheader("çº¢é»‘æ¦œ (Red/Black List)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»†åˆ†ç»´åº¦çš„åˆ—
        granular_cols = ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·'] 
        if 'å¹¿å‘Šç»„id' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„id')
        if 'å¹¿å‘Šç³»åˆ—' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç³»åˆ—')
        if 'å¹¿å‘Šç»„' in final_df.columns:
            granular_cols.append('å¹¿å‘Šç»„')
            
        # èšåˆæ•°æ®
        granular_perf = final_df.groupby(granular_cols).agg({'è´¹ç”¨': 'sum', 'è½¬åŒ–ä»·å€¼': 'sum'}).reset_index()
        granular_perf['ROAS'] = granular_perf.apply(lambda x: x['è½¬åŒ–ä»·å€¼'] / x['è´¹ç”¨'] if x['è´¹ç”¨'] > 0 else 0, axis=1)
        
        # æ•´ç†åˆ—é¡ºåº
        display_order = [c for c in ['ä¼˜åŒ–å¸ˆ', 'ç±»ç›®', 'å¹¿å‘Šè´¦å·', 'å¹¿å‘Šç³»åˆ—', 'å¹¿å‘Šç»„', 'å¹¿å‘Šç»„id', 'è´¹ç”¨', 'è½¬åŒ–ä»·å€¼', 'ROAS'] if c in granular_perf.columns]
        granular_perf = granular_perf[display_order]

        # æ”¹ä¸ºå‚ç›´æ’åˆ— (Vertical Layout)
        st.markdown("### ğŸ”´ äºæŸæ¦œ (æŒ‰æ¶ˆè€—é™åº - æ­¢æŸä¼˜å…ˆçº§)")
        st.markdown("âš ï¸ **æ­¢æŸå»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹æ¶ˆè€—é«˜ä¸” ROAS < 1.7ï¼Œåº”ä¼˜å…ˆæ£€æŸ¥ç´ ææˆ–å…³åœã€‚")
        
        # çº¢è‰²åˆ—è¡¨æŒ‰è´¹ç”¨é™åºæ’ï¼ˆäºæŸæœ€å¤šçš„æœ€å…ˆçœ‹ï¼‰
        red_list = granular_perf[(granular_perf['ROAS'] < 1.7) & (granular_perf['è´¹ç”¨'] > 0)].sort_values('è´¹ç”¨', ascending=False).head(20)
        st.dataframe(
            red_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                          .background_gradient(subset=['è´¹ç”¨'], cmap="Reds"),
            use_container_width=True,
            hide_index=True
        )
        
        st.markdown("---") # Divider
        
        st.markdown("### ğŸŒŸ æ˜æ˜Ÿæ¦œ (æŒ‰ ROAS é™åº - æ‰©é‡ä¼˜å…ˆçº§)")
        st.markdown("ğŸ’¡ **æ‰©é‡å»ºè®®**: ä¸‹åˆ—å¹¿å‘Šå­é¡¹ ROAS > 2.0ï¼Œæ•ˆç‡æé«˜ï¼Œå¯åœ¨ä¿æŒç¨³å®šçš„å‰æä¸‹é€‚åº¦æ‰©é‡ã€‚")
        
        # é»‘è‰²/æ˜æ˜Ÿåˆ—è¡¨æŒ‰ ROAS é™åºæ’ï¼ˆæ•ˆç‡æœ€é«˜çš„æœ€å…ˆçœ‹ï¼‰
        black_list = granular_perf[granular_perf['ROAS'] > 2.0].sort_values('ROAS', ascending=False).head(20)
        st.dataframe(
            black_list.style.format({"ROAS": "{:.2f}", "è´¹ç”¨": "{:,.2f}", "è½¬åŒ–ä»·å€¼": "{:,.2f}"})
                            .background_gradient(subset=['ROAS'], cmap="Greens"),
            use_container_width=True,
            hide_index=True
        )

    with tab5:
        st.subheader("æ•°æ®ä»“åº“ (Data Warehouse)")
        st.dataframe(final_df)

if __name__ == "__main__":
    main()
